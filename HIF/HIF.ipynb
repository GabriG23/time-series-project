{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHGp5QXK-WIJ"
      },
      "source": [
        "# Hybrid Isolation Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTHVNoFgf7CS"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yOX7q0vfM1U"
      },
      "outputs": [],
      "source": [
        "!pip install pandas==1.5.3\n",
        "!pip install tsfel\n",
        "!pip3 install --upgrade --no-cache-dir gdown       # support for download a large file from Google Drive\n",
        "!pip install numpy>=1.19.5\n",
        "!pip install scikit-learn>=0.24.1\n",
        "!pip install tadpak"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTUQmQfVf-ju"
      },
      "source": [
        "### Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xR5xgREIB4B7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wm3TOLrsB5FQ"
      },
      "outputs": [],
      "source": [
        "# unzip from drive\n",
        "!unzip /content/drive/MyDrive/Colab_MLA/MLA_Project/csv_20220811.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq3mEim7f_gL"
      },
      "outputs": [],
      "source": [
        "# Download from link\n",
        "import os, sys\n",
        "# https://drive.google.com/file/d/1Fn_KVRpwLedTYU1QgfVCRtkvo1hf_9GB/view?usp=sharing first account\n",
        "# https://drive.google.com/file/d/1P8pCKLI-64_HT91Oqid4RUGtZCUht2c-/view?usp=sharing second account\n",
        "\n",
        "if not os.path.isfile('/content/csv_20220811.zip'):\n",
        "  !gdown 1P8pCKLI-64_HT91Oqid4RUGtZCUht2c-\n",
        "  !jar xvf  \"/content/csv_20220811.zip\"\n",
        "\n",
        "if not os.path.isdir('/content/csv_20220811'):\n",
        "  print(\"Dataset doesn't exist\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKHHdKqMgEkL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import warnings\n",
        "import datetime\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "import pickle\n",
        "import random as rn\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "from collections import Counter\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, top_k_accuracy_score, f1_score, roc_curve, auc, precision_recall_curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noc-30ZFgFij"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUeSPi8jgFrE"
      },
      "outputs": [],
      "source": [
        "ROOTDIR_DATASET_NORMAL = \"/content/csv_20220811\"\n",
        "plt.style.use(\"Solarize_Light2\") # Set style for matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGftbXJcs8P-"
      },
      "source": [
        "##### Loading metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnYU--2ysz9G"
      },
      "outputs": [],
      "source": [
        "def read_folder_normal(dataset_folder, frequency):\n",
        "    ROOTDIR_DATASET = dataset_folder\n",
        "\n",
        "    filepaths_csv = [os.path.join(ROOTDIR_DATASET, f\"rec{r}_20220811_rbtc_{1/frequency}s.csv\") for r in [0, 2, 3, 4]]\n",
        "    filepaths_meta = [os.path.join(ROOTDIR_DATASET, f\"rec{r}_20220811_rbtc_{1/frequency}s.metadata\") for r in [0, 2, 3, 4]]\n",
        "\n",
        "    dfs = [pd.read_csv(filepath_csv, sep=\";\") for filepath_csv in filepaths_csv]\n",
        "    df = pd.concat(dfs)\n",
        "    df = df.sort_index(axis=1)\n",
        "    df.index = pd.to_datetime(df.time.astype('datetime64[ms]'), format=\"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "\n",
        "    columns_to_drop = [column for column in df.columns if \"Abb\" in column or \"Temperature\" in column]\n",
        "    df.drop([\"machine_nameKuka Robot_export_active_energy\", \"machine_nameKuka Robot_import_reactive_energy\"] + columns_to_drop, axis=1, inplace=True)\n",
        "\n",
        "    df.drop(['time'], axis=1, inplace=True)     # remove the last column time\n",
        "    X_train = df\n",
        "    return X_train\n",
        "\n",
        "\n",
        "def read_folder_collisions(dataset_folder, frequency):\n",
        "    ROOTDIR_DATASET = dataset_folder\n",
        "    collisions = pd.read_excel(os.path.join(ROOTDIR_DATASET, \"20220811_collisions_timestamp.xlsx\"))\n",
        "    collisions['Timestamp'] = collisions['Timestamp'] - pd.to_timedelta(2, 'h')\n",
        "\n",
        "    start_col = collisions[collisions['Inizio/fine'] == \"i\"][['Timestamp']].rename(columns={'Timestamp': 'start'})\n",
        "    end_col = collisions[collisions['Inizio/fine'] == \"f\"][['Timestamp']].rename(columns={'Timestamp': 'end'})\n",
        "\n",
        "    start_col.reset_index(drop=True, inplace=True)\n",
        "    end_col.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    df_collision = pd.concat([start_col, end_col], axis=1)\n",
        "\n",
        "    filepath_csv_test = [os.path.join(ROOTDIR_DATASET, f\"rec{r}_collision_20220811_rbtc_{1/frequency}s.csv\") for r in [1, 5]]\n",
        "    dfs_test = [pd.read_csv(filepath_csv, sep=\";\") for filepath_csv in filepath_csv_test]\n",
        "    df_test = pd.concat(dfs_test)\n",
        "\n",
        "    df_test = df_test.sort_index(axis=1)\n",
        "    df_test.index = pd.to_datetime(df_test.time.astype('datetime64[ms]'), format=\"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "    columns_to_drop = [column for column in df_test.columns if \"Abb\" in column or \"Temperature\" in column]\n",
        "    df_test.drop([\"machine_nameKuka Robot_export_active_energy\", \"machine_nameKuka Robot_import_reactive_energy\"] + columns_to_drop, axis=1, inplace=True)\n",
        "\n",
        "    df_test['time'] = pd.to_datetime(df_test['time'].astype('datetime64[ms]'), format=\"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "\n",
        "    X_collisions = df_test.drop(['time'], axis=1, inplace=False)\n",
        "    return df_collision, X_collisions, df_test\n",
        "\n",
        "def pre_processing(data, corr_features = None):\n",
        "\n",
        "  data = data.drop((data.columns[data.isna().any()].tolist()), axis = 1)  # Remove nan values\n",
        "\n",
        "  scaler = preprocessing.MinMaxScaler() # Normalizing\n",
        "  scaler.fit(data)\n",
        "  data = pd.DataFrame(scaler.transform(data), columns=data.columns)\n",
        "\n",
        "  selector_variance = VarianceThreshold() # Remove zero-variance\n",
        "  selector_variance.fit(data)\n",
        "  data = pd.DataFrame(selector_variance.transform(data), columns=data.columns.values[selector_variance.get_support()])\n",
        "\n",
        "  if corr_features == None:               # Remove highly correlated features\n",
        "    corr_features = tsfel.correlated_features(data, threshold=0.95)\n",
        "  data.drop(corr_features, inplace=True, axis=1)\n",
        "\n",
        "  return data, corr_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRmu8PNptGVU"
      },
      "source": [
        "### Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpmBp3XstGcE"
      },
      "outputs": [],
      "source": [
        "class hiForest(object):\n",
        "\n",
        "    def __init__(self, X_Train, ntrees, sample_size, limit = None):\n",
        "        self.ntrees = ntrees            # Number of trees\n",
        "        self.X = X_Train                # Training data shape [data point, features]\n",
        "        self.nobjs = len(X_Train)       # Number of nodes\n",
        "        self.sample = sample_size       # Dimension of the tree (elements of the tree)\n",
        "        self.Trees = []                 # Trees list\n",
        "        self.limit = limit              # Maximum depth of each tree\n",
        "\n",
        "        if limit is None:               # depth\n",
        "            self.limit = int(np.ceil(1.2 * np.log2(self.sample))) # based on the tree dimension, compute log2\n",
        "                                                                # multiply it for 1.2 to fix the logarithm\n",
        "                                                                # np.ceil: return the upper bound of the number: -1.7 -> -1, 1.7 --> 2\n",
        "        self.c = c_factor(self.sample)      # compute the c_factor for a given sample size\n",
        "\n",
        "        for i in range(self.ntrees):            # loop for each tree\n",
        "            ix = rn.sample(range(self.nobjs), self.sample) # Generate between the range of the time series a random number of element equal to sample size\n",
        "                                                           # ix: random sample_size indexes, it will be our subset\n",
        "            X_p = X_Train[ix]                              # given the index select the elements\n",
        "            self.Trees.append(hiTree(X_p, 0, self.limit))  # Save and create the tree given the values and the depth limit\n",
        "\n",
        "    def computeAggScore(self, x):\n",
        "        S = np.zeros(self.ntrees)           # array of scores: one for each tree\n",
        "        labsCount = Counter([])             # counter of labels (anomalies)\n",
        "        ldist = []                          # list for distance path\n",
        "        ldist_a = []                        # list for distance path to the anomalies centroid\n",
        "        for j in range(self.ntrees):            # loop for all the trees\n",
        "            pf = PathFactor(x, self.Trees[j])   # compute the path factor given x(data point) and the j-tree\n",
        "            path =  pf.path * 1.0               # extract the value from the path\n",
        "            S[j] = 2.0**(-1.0 * path/self.c)    # Compute the isolation forest score (first score)\n",
        "            labsCount = labsCount + pf.labs     # Update the label count\n",
        "\n",
        "            if(len(pf.ldist) > 0):                          # if the path distance of the datapoint is positive\n",
        "                ldist.append(np.mean(pf.ldist))             # save the mean of the distance path\n",
        "\n",
        "            if(len(pf.ldist_a) > 0):                        # if the path distance of the datapoint from the anomalies is positive\n",
        "                ldist_a.append(np.mean(pf.ldist_a, axis=0)) #  save the mean of the anomalies path\n",
        "\n",
        "        meanDist = 0                        # mean distance\n",
        "        if(len(ldist) > 0):\n",
        "            meanDist = np.mean(ldist)       # mean of distance path (second score)\n",
        "\n",
        "        meanDist_r = 0\n",
        "        if(len(ldist_a) > 0):\n",
        "            meanDist_a = np.mean(ldist_a, axis=0)    # mean of anomalies distance path\n",
        "            if(meanDist_a > 0):\n",
        "                meanDist_r = meanDist / (meanDist_a)    # relative mean (third score)\n",
        "\n",
        "        return np.mean(S), labsCount, meanDist, meanDist_r  # return 1st score, labels count, 2nd score, 3rd score\n",
        "\n",
        "\n",
        "    def addAnomaly(self, x, lab):                       # ann an anomalie in the tree\n",
        "        for j in range(self.ntrees):                    # loop in each tree\n",
        "            pf = PathFactor(x, self.Trees[j])           # create the path factor for the data point and the given tree\n",
        "            pf.addAnomaly(x, lab, self.Trees[j].root)   # add the anomaly on the root node\n",
        "\n",
        "    def computeAnomalyCentroid(self):                       # compute the anomaly centroid\n",
        "        for j in range(self.ntrees):                        # loop for each tree\n",
        "            self.Trees[j].root.computeAnomalyCentroid()     # compute the Anomaly centroid for the root node\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzE-fBCstH9c"
      },
      "source": [
        "### Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZtEjcIStIBe"
      },
      "outputs": [],
      "source": [
        "class hiTree(object):\n",
        "\n",
        "    \"\"\"\n",
        "    Unique entries for X\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, e, l): # X input samples, e = 0, l = depth limit\n",
        "        self.e = e               # actual depth\n",
        "        self.X = X               # save input data samples\n",
        "        self.size = len(X)       # size of X (number of samples) [samples, features]\n",
        "        self.Q = np.arange(np.shape(X)[1], dtype='int') # Q features aranged\n",
        "        self.l = l               # depth limit\n",
        "        self.p = None            # random value between min and max from the feature chosen (threshold)\n",
        "        self.q = None            # random index of X-features\n",
        "        self.exnodes = 0         # external nodes\n",
        "        self.labs = []           # labels\n",
        "        self.root = self.make_tree(X, e, l) # creating the tree given X samples, depth and limit\n",
        "\n",
        "    def make_tree(self, X, e, l):\n",
        "        self.e = e      # assign the depth\n",
        "        if e >= l or len(X) <= 1: # check if depth is greater than limit or we don't have anymore samples\n",
        "            left = None           # create an EXTERNAL empty node with left and rigth equal to None\n",
        "            right = None\n",
        "            self.exnodes += 1     # increase the number of external node\n",
        "            return Node(X, self.q, self.p, e, left, right, node_type = 'exNode')\n",
        "        else:\n",
        "            self.q = rn.choice(self.Q)  # select a random index from Q from 1 to number of features\n",
        "            self.p = rn.uniform(X[:, self.q].min(), X[:, self.q].max())\n",
        "                                # X[:, self.q] extract the values from the q-features. In this case it will be equal to the values of the data points for that features\n",
        "                                # .min return the minimum value\n",
        "                                # .max return the maximum value\n",
        "                                # rn.uniform(min, max): generate a random value from that interval\n",
        "\n",
        "            w = np.where(X[:, self.q] < self.p, True, False)  # mask\n",
        "                # create an array w of true/false based on the check\n",
        "                # X[:, self.q] extract the values from the column given by q\n",
        "                # < self.p: if the value is lower than the random value chosen by p assign True otherwise False\n",
        "                # w sarà un array dove ogni elemento è true se la condizione è verificata altrimenti False\n",
        "\n",
        "            return Node(X, self.q, self.p, e, left=self.make_tree(X[w], e + 1, l), right=self.make_tree(X[~w], e + 1, l), node_type = 'inNode' )\n",
        "                   # create a new node with\n",
        "                   # X input samples\n",
        "                   # q: feature index on which the node is split, p: threshold for the split (random value between min and max)\n",
        "                   # e: current depth, node_type: inNode or exNode\n",
        "                   # left: left child created with the true value of X selected with the mask w (lower values)\n",
        "                   # right: right child created with the false value (greater values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai9XZdfrtUhi"
      },
      "source": [
        "### Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFoL4fBhtUmy"
      },
      "outputs": [],
      "source": [
        "class Node(object):\n",
        "    def __init__(self, X, q, p, e, left, right, node_type = '' ):\n",
        "        self.e = e              # node depth\n",
        "        self.size = len(X)      # len of x, sample_size\n",
        "        self.q = q              # feature index on thich the node is split\n",
        "        self.p = p              # threshold value for the split\n",
        "        self.left = left        # left and right child\n",
        "        self.right = right\n",
        "        self.ntype = node_type  # 'exNode' for leaf nodes and 'inNode' for inside nodes\n",
        "        self.C = None           # Centroid of the node\n",
        "        self.Ca = None          # Centroid of the anomaly\n",
        "        self.labs = []          # list of labels, used for the anomalies of the external node\n",
        "        self.Xanomaly = []      # list to memorize anomalies in the 'exNode'\n",
        "        if(node_type == 'exNode' and self.size > 0):  # if it is an extern node and the size is positive\n",
        "            self.C = np.mean(X, axis=0)               # centroid of that Node, computed with the mean of the element of X\n",
        "\n",
        "    def computeAnomalyCentroid(self):\n",
        "        if self.ntype == 'exNode':                        # check for extern node\n",
        "            if(len(self.Xanomaly) > 0):                   # check if some anomalies have been added to this exNode\n",
        "                self.Ca = np.mean(self.Xanomaly, axis=0)  # compute mean of anomalies\n",
        "        else:\n",
        "            self.left.computeAnomalyCentroid()            # otherwise go deep left and right until you find some anomalies\n",
        "            self.right.computeAnomalyCentroid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jpfa5HltYmr"
      },
      "source": [
        "### Path Factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIMxPsPbtYpr"
      },
      "outputs": [],
      "source": [
        "class PathFactor(object):\n",
        "    def __init__(self, x, hitree):  # I receive a data point x and a tree\n",
        "        self.path_list = []         # path list\n",
        "        self.labs = []              # labels\n",
        "        self.ldist = []             # list of distance path\n",
        "        self.ldist_a = []           # list of anomalies path\n",
        "        self.x = x                  # data point\n",
        "        self.e = 0                  # depth\n",
        "        self.path = self.find_path(hitree.root)   # find the path from the root\n",
        "\n",
        "    def find_path(self, T):\n",
        "        if T.ntype == 'exNode':             # if it is a external node\n",
        "            self.labs = Counter(T.labs)     # count the labels of the exNode\n",
        "            if not (T.C is None):\n",
        "                self.ldist.append(EuclideanDist(self.x, T.C))    # compute the Euclidean distance of the centroid\n",
        "            if not (T.Ca is None):\n",
        "                self.ldist_a.append(EuclideanDist(self.x, T.Ca)) # compute the Euclidean distance of anomaly centroid\n",
        "            sz = T.size     # tree dimension equal to the number of samples\n",
        "            if(sz == 0):    # if equals to zero increase by 1\n",
        "                sz += 1\n",
        "            for key in self.labs:           # loop between the labels of exNode (we have only 1 type of anomalies)\n",
        "                self.labs[key] /= sz        # normalize the label count by dividing its associated value with the value of size\n",
        "\n",
        "            if T.size == 1:                 # if there's only 1 element\n",
        "                return self.e               # return depth\n",
        "            else:\n",
        "                self.e = self.e + c_factor(T.size)  # return depth with c_factor\n",
        "                return self.e\n",
        "        else:                                 # if it is a inNode\n",
        "            a = T.q                           # a is our index features\n",
        "            self.e += 1                       # update depth\n",
        "            if self.x[a] < T.p:               # check if the featutes of our datapoint are lower than our threshold\n",
        "                self.path_list.append('L')    # save LEFT and go down left\n",
        "                return self.find_path(T.left)\n",
        "            else:\n",
        "                self.path_list.append('R')    # save RIGHT and go down right\n",
        "                return self.find_path(T.right)\n",
        "\n",
        "    def addAnomaly(self, x, lab, T):\n",
        "        if T.ntype == 'exNode':     # check if it's and external node\n",
        "            T.labs.append(lab)      # save the label (anomaly = 1)\n",
        "            T.Xanomaly.append(x)    # save the value of the anomaly\n",
        "        else:\n",
        "            a = T.q                 # index of the selected features\n",
        "            if self.x[a] < T.p:     # check with the threshold\n",
        "                return self.addAnomaly(x, lab, T.left)    # go deep left until you find the exNode\n",
        "            else:\n",
        "                return self.addAnomaly(x, lab, T.right)   # go deep right until you find the exNode\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nk1TzXytikf"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZeEjT7utipB"
      },
      "outputs": [],
      "source": [
        "def EuclideanDist(x, y):\n",
        "    return np.sqrt(np.sum((x - y) ** 2))\n",
        "    # (x - y) ** 2: subtract the two array and raise them to the power of 2\n",
        "    # np.sum((x - y) ** 2): compute the sum of square subtraction between the two array\n",
        "    # np.sqrt: square root of the sum\n",
        "\n",
        "# example: if n = 1024 --> c = 13.017 / n = 512 --> 11.631\n",
        "def c_factor(n): # this factor is used to estimate the number of permutation in combinatory problems\n",
        "    if(n < 2):   # check if input is lower than 2\n",
        "        n = 2\n",
        "    return 2.0 * (np.log(n - 1) + 0.5772156649) - (2.0 * (n - 1.) / (n * 1.0))\n",
        "\n",
        "# The Average path length of unsuccessful search in BTS as:\n",
        "#       c(n) = 2 * H(n - 1) - (2 * (n - 1) / n)\n",
        "#       where: H(i) is the harmonic number and that can be estimated by: log(i) + 0.5772156649 (Euler's constant)\n",
        "#       as c(n) is the average of h(x) given n, we use it to normalize h(x). The anomaly score s of an istance x is defined as: ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypTS64sHgtqZ"
      },
      "source": [
        "### Training forest function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nB2cxHCugtvs"
      },
      "outputs": [],
      "source": [
        "def training_forest(X_train, n_trees, max_samples, forest_type, min_samples_size, X_collisions_add = None, df_test_add = None, df_collision = None):\n",
        "\n",
        "  if max_samples: # setting max_samples\n",
        "      sample_size = int(max_samples * X_train.shape[0])\n",
        "  else:\n",
        "      sample_size = min(min_samples_size, X_train.shape[0])  # if max_samples is None\n",
        "\n",
        "  X_train_norm = X_train.values\n",
        "\n",
        "  # UNSUPERVISED extension that exploits a distance knowledge to neighboring 'normal' data\n",
        "  Forest = hiForest(X_train_norm, n_trees, sample_size)\n",
        "\n",
        "  if forest_type == 'supervised':\n",
        "      # SUPERVISED-based extention: if we want we can add anomalies in the Forest\n",
        "      tot_anomalies = 0       # anomalie totali\n",
        "      index_anomaly = []      # anomalies index\n",
        "      idx = 0\n",
        "\n",
        "      for _, row in df_test_add.iterrows():                 # take row from df_test\n",
        "          for _, collision_row in df_collision.iterrows():  # take the interval from df_collision\n",
        "              if (row['time'] >= collision_row['start']) and (row['time'] <= collision_row['end']): # check if the row belongs to the interval\n",
        "                  tot_anomalies += 1                # increase the total anomalies added\n",
        "                  index_anomaly.append(idx)         # save the index\n",
        "\n",
        "                  anomaly_to_add = row.drop(['time'], axis=0, inplace=False)  # drop the time from the row\n",
        "                  anomaly_to_add = anomaly_to_add.values                      # convert in numpy\n",
        "                  Forest.addAnomaly(anomaly_to_add, lab=1)                    # add the anomaly, label = 1 (we only have 2 label, normal data and anomalies)\n",
        "          idx += 1\n",
        "      print(f\"Anomalies detected: {tot_anomalies}\")\n",
        "\n",
        "  Forest.computeAnomalyCentroid()\n",
        "  print(\"Forest created with success.\")\n",
        "  return Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fbg7GJkKi16o"
      },
      "outputs": [],
      "source": [
        "def aggregate_scores(s_if, s_unsupervised, s_supervised, alpha1, alpha2):\n",
        "\n",
        "  # normalize the scores and check if they are equal to 0 to avoid division by 0\n",
        "  if np.max(s_if) - np.min(s_if) == 0:\n",
        "    s_if_norm = np.zeros_like(s_if)\n",
        "  else:\n",
        "    s_if_norm = (s_if - np.min(s_if)) / (np.max(s_if) - np.min(s_if))\n",
        "\n",
        "  if np.max(s_unsupervised) - np.min(s_unsupervised) == 0:\n",
        "    s_unsupervised_norm = np.zeros_like(s_unsupervised)\n",
        "  else:\n",
        "    s_unsupervised_norm = (s_unsupervised - np.min(s_unsupervised)) / (np.max(s_unsupervised) - np.min(s_unsupervised))\n",
        "\n",
        "  if np.max(s_supervised) - np.min(s_supervised) == 0:\n",
        "    s_supervised_norm = np.zeros_like(s_supervised)\n",
        "  else:\n",
        "    s_supervised_norm = (s_supervised - np.min(s_supervised)) / (np.max(s_supervised) - np.min(s_supervised))\n",
        "\n",
        "  scores = alpha2 * (alpha1 * s_if_norm + (1 - alpha1) * s_unsupervised_norm) + (1 - alpha2) * s_supervised_norm\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttCVykuLi7jE"
      },
      "outputs": [],
      "source": [
        "def score_hif(Forest, X_collisions, score_type):\n",
        "  X_collisions_norm = X_collisions.values     # convert the values\n",
        "  size = X_collisions_norm.shape[0]           # size of the scores\n",
        "\n",
        "  scores_if = np.zeros(size)  # path isolation forest\n",
        "  scores_un = np.zeros(size)  # unsupervised\n",
        "  scores_s = np.zeros(size)   # supervised\n",
        "\n",
        "  alpha1 = 0.3                # best results between [0.2, 0.5]\n",
        "\n",
        "  if score_type == 'unsupervised':\n",
        "     alpha2 = 1               # with alpha2 = 1 we don't need the supervised score\n",
        "  else:\n",
        "     alpha2 = 0.7             #  best results between [0.6, 0,9]\n",
        "\n",
        "  for i in range(size):       # loop for each point\n",
        "      scorePath, _, meanDist, meanDistA = Forest.computeAggScore(X_collisions_norm[i])    # compute scores and save them\n",
        "      scores_if[i] = scorePath\n",
        "      scores_un[i] = meanDist\n",
        "      scores_s[i] = meanDistA\n",
        "\n",
        "  scores = aggregate_scores(scores_if, scores_un, scores_s, alpha1, alpha2)               # aggregate the scores\n",
        "  #anomaly_scores_norm = (scores - np.min(scores)) / (np.max(scores) - np.min(scores))     # normalize the scores\n",
        "  print(\"Scores computed with success.\")\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7i67AI7L564"
      },
      "source": [
        "### Variables init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ts0uPJ1Vvy4J"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEvIe7XOg3_Q"
      },
      "outputs": [],
      "source": [
        "frequency = 10\n",
        "max_samples = None\n",
        "n_trees = 1024\n",
        "min_samples_size = 256\n",
        "X_train = read_folder_normal(ROOTDIR_DATASET_NORMAL, frequency) # n_row, n_features (90k, 55), 56 with time\n",
        "df_collision, X_collisions, df_test = read_folder_collisions(ROOTDIR_DATASET_NORMAL, frequency) # (95815, 55) test data extraction\n",
        "\n",
        "X_train, corr_features = pre_processing(X_train)               # (95815, 55)\n",
        "X_collisions, _ = pre_processing(X_collisions, corr_features)\n",
        "df_test.drop(corr_features, inplace=True, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWrtEbIgiEk_"
      },
      "source": [
        "##### Split for adding anomalies and then testing (only for supervised hif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g37zatXkh9Ix"
      },
      "outputs": [],
      "source": [
        "anomaly_split_number = 0.02 # 2%-3% del dataset\n",
        "split_at = int(len(X_collisions) * anomaly_split_number)  # range dello split\n",
        "\n",
        "X_collisions_add = X_collisions[:split_at]   # (685, 55)             # le anomalie in questo range verranno aggiunte nella foresta\n",
        "X_collisions = X_collisions[split_at:]  # (33590, 55)\n",
        "\n",
        "df_test_add = df_test[:split_at]                  # (685, 56)\n",
        "df_test = df_test[split_at:]                 # (33590, 56)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G8ZDoHKteP0"
      },
      "source": [
        "### Training Forest Unsupervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmmjrT7lesFT"
      },
      "outputs": [],
      "source": [
        "model_title = 'HIF_unsupervised'\n",
        "Forest = training_forest(X_train, n_trees, max_samples, 'unsupervised', min_samples_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mU_ol0WjTZP"
      },
      "outputs": [],
      "source": [
        "scores = score_hif(Forest, X_collisions, 'unsupervised') ##### Computing Scores Forest Unsupervised (no anomalies added)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T6wYG1C-imj"
      },
      "source": [
        " ### Training Forest Supervised (with anomaly split)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6yr5ajMjAB9"
      },
      "outputs": [],
      "source": [
        "model_title = 'HIF_supervised'\n",
        "Forest = training_forest(X_train, n_trees, max_samples, 'supervised', min_samples_size, X_collisions_add, df_test_add, df_collision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt8g0Z8_jUXj"
      },
      "outputs": [],
      "source": [
        "scores = score_hif(Forest, X_collisions, 'supervised') ##### Computing Scores Forest Supervised (we added anomalies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBby3YaSNGlG"
      },
      "source": [
        "### Evaluate functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXVecrmQNGpk"
      },
      "outputs": [],
      "source": [
        "# plot distribution and return the true_labels\n",
        "def plot_hist(anomaly_scores, df_collision, df, title):\n",
        "    index_anomaly = []      # anomalies' index\n",
        "    idx = 0\n",
        "    for _, row in df.iterrows():\n",
        "        for _, collision_row in df_collision.iterrows():\n",
        "            if (row['time'] >= collision_row['start']) and (row['time'] <= collision_row['end']):\n",
        "                index_anomaly.append(idx)\n",
        "        idx += 1\n",
        "    true_labels = np.zeros_like(anomaly_scores)\n",
        "    true_labels[index_anomaly] = 1\n",
        "    logging.info(f\"Anomalies detected: {int(true_labels.sum())}\")\n",
        "    anomaly_values = anomaly_scores[index_anomaly]\n",
        "    normal_values = np.delete(anomaly_scores, index_anomaly)\n",
        "\n",
        "    plt.hist(normal_values, bins=30, color=\"tab:blue\", ec=\"dodgerblue\", alpha=0.5, label='Normal')\n",
        "    plt.hist(anomaly_values, bins=30, color='tab:red', ec=\"darkred\", alpha=0.7, label='Anomalies')\n",
        "\n",
        "    plt.xlabel('Values')\n",
        "    plt.ylabel('Occurrencies')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title(title)\n",
        "    plt.savefig(f'/content/{title}.jpg')  # Modify the path and filename as needed\n",
        "    plt.show()\n",
        "    return true_labels\n",
        "\n",
        "# compute f1, fB score, auc-roc, auc-pr\n",
        "def compute_metrics(anomaly_scores_norm, df_test, y_true, th=None):\n",
        "    tot_anomalies = y_true.sum()\n",
        "    sens = list()           # recalls o tpr\n",
        "    spec = list()\n",
        "    fpr = list()\n",
        "    f1 = list()\n",
        "    f0_1= list()\n",
        "    prec = list()\n",
        "    cm_list = list()\n",
        "    anomlay_indexes_dict = dict()\n",
        "    acc_with_err = list()\n",
        "    step = 0.01\n",
        "    ths = np.arange(0, 1, step)\n",
        "    if th is None:\n",
        "        for threshold in tqdm(ths):\n",
        "            anomalies_pred = anomaly_scores_norm > threshold\n",
        "            tp = 0                                                          # true positive per quella threshold\n",
        "            anomaly_indexes = list()\n",
        "            for index, anomaly_pred in enumerate(anomalies_pred):\n",
        "                if y_true[index] and anomaly_pred:\n",
        "                    anomaly_indexes.append(index)\n",
        "                    tp += 1\n",
        "\n",
        "            cm_anomaly = np.zeros((2,2))\n",
        "            n_sample = len(df_test)\n",
        "            n_not_collision = n_sample - tot_anomalies\n",
        "            n_detected = anomalies_pred.sum()\n",
        "\n",
        "            fp = n_detected - tp\n",
        "            fn = tot_anomalies - tp\n",
        "            tn = n_not_collision - fp\n",
        "\n",
        "            cm_anomaly[0, 0] = tn\n",
        "            cm_anomaly[0, 1] = fp\n",
        "            cm_anomaly[1, 0] = fn\n",
        "            cm_anomaly[1, 1] = tp\n",
        "\n",
        "            cm_list.append(cm_anomaly)\n",
        "            recall = tp / (tp + fn)\n",
        "            sens.append(recall)\n",
        "            fpr.append(1 - tn /(tn + fp))\n",
        "            precision = tp / (tp + fp)\n",
        "            prec.append(precision)\n",
        "            spec.append(tn /(tn + fp))\n",
        "            f1.append(2 * tp / (2 * tp + fp + fn))\n",
        "            f0_1.append((1 + 0.1**2) * tp / ((1 + 0.1**2) * tp +  0.1**2*fp + fn))\n",
        "            cm_anomaly_norm = cm_anomaly.astype('float') / cm_anomaly.sum(axis=1)[:, np.newaxis]\n",
        "            acc_with_err.append( (np.mean(np.diag(cm_anomaly_norm)), np.std(np.diag(cm_anomaly_norm))) )\n",
        "            anomlay_indexes_dict[threshold] = anomaly_indexes\n",
        "\n",
        "        f1_max = max(f1)\n",
        "        f0_1_max = max(f0_1)\n",
        "        max_index_f1 = f1.index(f1_max)\n",
        "        max_index_f0_1 = f0_1.index(f0_1_max)\n",
        "        th_f1_max = max_index_f1 * step\n",
        "        th_f0_1_max = max_index_f0_1 * step\n",
        "        print(f\"f1: {f1_max} at th: {th_f1_max}\")\n",
        "        print(f\"f0.1: {f0_1_max} at th: {th_f0_1_max}\")\n",
        "        print(f\"AUC-PR: {metrics.average_precision_score(y_true, anomaly_scores_norm)}\")\n",
        "        print(f\"AUC-ROC: {metrics.roc_auc_score(y_true, anomaly_scores_norm)}\")\n",
        "        return sens, fpr, th_f1_max\n",
        "    else:\n",
        "        df_anomaly = df_test.loc[np.array(anomaly_scores_norm > th)]\n",
        "        tp = 0                                                          # true positive per quella threshold\n",
        "        anomaly_indexes = list()\n",
        "        anomalies_pred = anomaly_scores_norm > th\n",
        "\n",
        "        for index, anomaly_pred in enumerate(anomalies_pred):\n",
        "            if y_true[index] and anomaly_pred:\n",
        "                anomaly_indexes.append(index)\n",
        "                tp += 1\n",
        "\n",
        "        cm_anomaly = np.zeros((2,2))\n",
        "        n_sample = len(df_test)\n",
        "        n_not_collision = n_sample - tot_anomalies\n",
        "        n_detected = len(df_anomaly)\n",
        "\n",
        "        fp = n_detected - tp\n",
        "        fn = tot_anomalies - tp\n",
        "        tn = n_not_collision - fp\n",
        "\n",
        "        cm_anomaly[0, 0] = tn\n",
        "        cm_anomaly[0, 1] = fp\n",
        "        cm_anomaly[1, 0] = fn\n",
        "        cm_anomaly[1, 1] = tp\n",
        "\n",
        "        f1 = 2 * tp / (2 * tp + fp + fn)\n",
        "        f0_1 = (1 + 0.1**2) * tp / ((1 + 0.1**2) * tp +  0.1**2*fp + fn)\n",
        "        print(f\"f1: {f1} at th: {th} for the test set\")\n",
        "        print(f\"f0.1: {f0_1} at th: {th} for the test set\")\n",
        "\n",
        "# another way to compute true_labels\n",
        "def create_true_labels(df_test, df_collision, scores):\n",
        "    index_anomaly = []\n",
        "    idx = 0\n",
        "    for _, row in df_test.iterrows():    # prende la riga da df_validation\n",
        "        for _, collision_row in df_collision.iterrows():  # prende la collision da df_collision\n",
        "            if (row['time'] >= collision_row['start']) and (row['time'] <= collision_row['end']):\n",
        "                index_anomaly.append(idx)         # salva l'indice\n",
        "        idx += 1               # aumenta l'indice\n",
        "    true_labels = np.zeros_like(scores)\n",
        "    true_labels[index_anomaly] = 1\n",
        "    logging.info(f\"Anomalies detected: {int(true_labels.sum())}\")\n",
        "    return true_labels\n",
        "\n",
        "# dataset divition for testing with validation\n",
        "def dataset_div(X_collisions, anomaly_scores_norm, df_test):\n",
        "  split = 0.9                                    # splitting value\n",
        "  split_at = int(len(X_collisions) * split)      # elements\n",
        "\n",
        "  asn_val = anomaly_scores_norm[split_at:]       # validation scores\n",
        "  asn_col = anomaly_scores_norm[:split_at]       # test scores\n",
        "\n",
        "  df_val = df_test.iloc[split_at:]\n",
        "  df_col = df_test.iloc[:split_at]\n",
        "\n",
        "  df_val = df_val[-asn_val.shape[0]:]\n",
        "  df_col = df_col[-asn_col.shape[0]:]\n",
        "\n",
        "  return df_val, df_col, asn_val, asn_col"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzT3xKxiMDp6"
      },
      "source": [
        "# Testing on a single model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El64hdEX5d76"
      },
      "source": [
        "##### Uploading scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10PT39bYy5xJ"
      },
      "outputs": [],
      "source": [
        "with open('/content/hif_unsupervised_f100_trees1024_sample256.pkl', \"rb\") as file:\n",
        "      scores_hif = pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrAGyGm95giU"
      },
      "source": [
        "##### Computing true_labels and metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hurzDvfyyln"
      },
      "outputs": [],
      "source": [
        "true_labels = plot_hist(scores, df_collision, df_test, title='HIF_distribution_f=10Hz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqr2RLqreL4x"
      },
      "outputs": [],
      "source": [
        "compute_metrics(scores, df_test, true_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNuRIKh16oFU"
      },
      "source": [
        "##### Testing with validation split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9MkYN0fMOcT"
      },
      "outputs": [],
      "source": [
        "df_val, df_col, asn_val, asn_col = dataset_div(X_collisions, scores, df_test)\n",
        "true_labels_val = plot_hist(asn_val, df_collision, df_val, title='HIF_supervised_Distribution_Val_f=10Hz')\n",
        "_, _, th_f1_max = compute_metrics(asn_val, df_val, true_labels_val)\n",
        "true_labels_test = plot_hist(asn_col, df_collision, df_col, title='HIF_supervised_Distribution_Test_f=10Hz')\n",
        "compute_metrics(asn_col, df_col, true_labels_test, th_f1_max)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUuA6Zhx5XY7"
      },
      "source": [
        "##### Downloading scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86RE7_gDMSiG"
      },
      "outputs": [],
      "source": [
        "anomaly_score = {\n",
        "            'anomaly_scores_norm' : scores,\n",
        "            'true_labels' : true_labels\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThHshiyAMU0K"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/result_dict.pkl', 'wb') as file:\n",
        "    pickle.dump(anomaly_score, file)\n",
        "files.download('/content/drive/MyDrive/result_dict.pkl')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
