{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e-lb6IO97Mf"
      },
      "source": [
        "# KMeans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE0bOfWuYSNf"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xY4l8jENYDZF"
      },
      "outputs": [],
      "source": [
        "!pip install pandas==1.5.3\n",
        "!pip install tsfel\n",
        "!pip3 install --upgrade --no-cache-dir gdown       # support for download a large file from Google Drive\n",
        "!pip install numpy>=1.19.5\n",
        "!pip install scikit-learn>=0.24.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "323mOkEPGEHC"
      },
      "source": [
        "### Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1smUodg1ZRWn"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbiecrYaYXW_"
      },
      "outputs": [],
      "source": [
        "# unzip from drive\n",
        "!unzip /content/drive/MyDrive/Colab_MLA/MLA_Project/csv_20220811.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwUpSuEuYRfH"
      },
      "outputs": [],
      "source": [
        "# or Download from link\n",
        "import os, sys\n",
        "# https://drive.google.com/file/d/1Fn_KVRpwLedTYU1QgfVCRtkvo1hf_9GB/view?usp=sharing first account\n",
        "# https://drive.google.com/file/d/1P8pCKLI-64_HT91Oqid4RUGtZCUht2c-/view?usp=sharing second account\n",
        "\n",
        "if not os.path.isfile('/content/csv_20220811.zip'):\n",
        "  !gdown 1P8pCKLI-64_HT91Oqid4RUGtZCUht2c-\n",
        "  !jar xvf  \"/content/csv_20220811.zip\"\n",
        "\n",
        "if not os.path.isdir('/content/csv_20220811'):\n",
        "  print(\"Dataset doesn't exist\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hnZe_wUoadOc"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import time\n",
        "import warnings\n",
        "import datetime\n",
        "import torch\n",
        "import tsfel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "import pickle\n",
        "import random as rn\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.base import BaseEstimator, OutlierMixin\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics\n",
        "from google.colab import files\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, top_k_accuracy_score, f1_score, roc_curve, auc, precision_recall_curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JsFPnjQbM93"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "C0OlcwbH-R_B"
      },
      "outputs": [],
      "source": [
        "ROOTDIR_DATASET_NORMAL = \"/content/csv_20220811\"\n",
        "plt.style.use(\"Solarize_Light2\") # Set style for matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSt8FMV1_pHu"
      },
      "source": [
        "##### Loading metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vDBxe_Lbleuj"
      },
      "outputs": [],
      "source": [
        "def get_metadata(filepaths_csv, filepaths_meta, action2int=None, delimiter=\";\"):\n",
        "  dfs_meta = list()\n",
        "  for filepath in filepaths_meta:                                   # read filepath (0, 2, 3, 4)\n",
        "        df_m = pd.read_csv(filepath, sep=delimiter)                 # read csv files\n",
        "        df_m.str_repr = df_m.str_repr.str.replace('True', 'true')   # replace True with true\n",
        "        df_m['filepath'] = filepath                                 # create the 'filepath' column\n",
        "        dfs_meta.append(df_m)                                       # add the corresponding dataframe\n",
        "  df_meta = pd.concat(dfs_meta)                                     # concatenate all dataframes\n",
        "\n",
        "  df_meta.index = pd.to_datetime(df_meta.init_timestamp.astype('datetime64[ms]'), format=\"%Y-%m-%dT%H:%M:%S.%f\")                        # convert numerical index in time index\n",
        "  df_meta['completed_timestamp'] = pd.to_datetime(df_meta.completed_timestamp.astype('datetime64[ms]'), format=\"%Y-%m-%dT%H:%M:%S.%f\")  # change format of completed_timestamp\n",
        "  df_meta['init_timestamp'] = pd.to_datetime(df_meta.init_timestamp.astype('datetime64[ms]'), format=\"%Y-%m-%dT%H:%M:%S.%f\")            # change format of init_timestamp\n",
        "\n",
        "  actions = df_meta.str_repr.unique()                                             # due to the concat of before we know take the actions removing the duplicate\n",
        "\n",
        "  dfs = [pd.read_csv(filepath_csv, sep=\";\") for filepath_csv in filepaths_csv]    # read the train/test datas\n",
        "  df = pd.concat(dfs)                                                             # concat the data like before\n",
        "  df = df.sort_index(axis=1)                                                      # sort columns by name\n",
        "\n",
        "  df.index = pd.to_datetime(df.time.astype('datetime64[ms]'), format=\"%Y-%m-%dT%H:%M:%S.%f\")        # set timestamp as index\n",
        "  columns_to_drop = [column for column in df.columns if \"Abb\" in column or \"Temperature\" in column] # remove uselesse columns\n",
        "  df.drop([\"machine_nameKuka Robot_export_active_energy\", \"machine_nameKuka Robot_import_reactive_energy\"] + columns_to_drop, axis=1, inplace=True)\n",
        "\n",
        "  df_action = list()        # take the actions\n",
        "  for action in actions:    # loop for each action (30)\n",
        "      for index, row in df_meta[df_meta.str_repr == action].iterrows(): # get index and row from metadata where the actions are the same\n",
        "          start = row['init_timestamp']         # start\n",
        "          end = row['completed_timestamp']      # end\n",
        "          df_tmp = df.loc[start: end].copy()    # temporary dataframe\n",
        "          df_tmp['action'] = action             # get action\n",
        "          df_tmp['duration'] = str((row['completed_timestamp'] - row['init_timestamp']).total_seconds())    # life of the action (it's not a feature)\n",
        "          df_action.append(df_tmp)\n",
        "  df_action = pd.concat(df_action, ignore_index=True)     # concatenate the actions\n",
        "  df_action.index = pd.to_datetime(df_action.time.astype('datetime64[ms]'), format=\"%Y-%m-%dT%H:%M:%S.%f\")   # set the time as index\n",
        "  df_action = df_action[~df_action.index.duplicated(keep='first')]     # keep the duplicate\n",
        "\n",
        "  df = df.dropna(axis=0)                  # remove NaNs from Df (34275, 56)\n",
        "  df_action = df_action.dropna(axis=0)    # (33063, 58)\n",
        "\n",
        "  if action2int is None:        # map the actions to integer --> 30 action - 30 indexes from 1 to 30\n",
        "      action2int = dict()\n",
        "      j = 1\n",
        "      for label in df_action.action.unique():\n",
        "          action2int[label] = j\n",
        "          j += 1\n",
        "\n",
        "  df_merged = df.merge(df_action[['action']], left_index=True, right_index=True, how=\"left\")  # (34275, 57)\n",
        "  df_idle = df_merged[df_merged['action'].isna()].copy()    # (1212, 57)\n",
        "  df_idle['action'] = 'idle'\n",
        "  df_idle['duration'] = df_action.duration.values.astype(float).mean().astype(str)\n",
        "  df_action = pd.concat([df_action, df_idle]) # (34275, 58)\n",
        "  action2int['idle'] = 0\n",
        "  return df_action, df, df_meta, action2int"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7YFKMlitN2x"
      },
      "source": [
        "##### Loading test and train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4oqwZ3Oko6EG"
      },
      "outputs": [],
      "source": [
        "frequency = 1    # 1 10 100 200 Hz - life {1: 10, 10: 1, 100: 0.1, 200: 0.05}\n",
        "\n",
        "filepath_csv_test = [os.path.join(ROOTDIR_DATASET_NORMAL, f\"rec{r}_collision_20220811_rbtc_{1/frequency}s.csv\") for r in [1, 5]]        # read data with anomalies\n",
        "filepath_meta_test = [os.path.join(ROOTDIR_DATASET_NORMAL, f\"rec{r}_collision_20220811_rbtc_{1/frequency}s.metadata\") for r in[1, 5]]\n",
        "\n",
        "filepath_csv_train = [os.path.join(ROOTDIR_DATASET_NORMAL, f\"rec{r}_20220811_rbtc_{1/frequency}s.csv\") for r in [0, 2, 3, 4]]           # read non-anomalous data\n",
        "filepath_meta_train = [os.path.join(ROOTDIR_DATASET_NORMAL, f\"rec{r}_20220811_rbtc_{1/frequency}s.metadata\") for r in [0, 2, 3, 4]]\n",
        "\n",
        "df_action_train, df_train, df_meta_train, action2int_train = get_metadata(filepath_csv_train, filepath_meta_train)    # read corresponding metadata\n",
        "df_action_test, df_test, df_meta_test, action2int_test = get_metadata(filepath_csv_test, filepath_meta_test)\n",
        "\n",
        "df_test['time'] = pd.to_datetime(df_test.time.astype('datetime64[ms]'), format =\"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "\n",
        "X_train = df_train.drop(['time'], axis=1, inplace=False)     # remove last column 'time' from dataset\n",
        "X_collisions = df_test.drop(['time'], axis=1, inplace=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPgsb0jitKRX"
      },
      "source": [
        "##### Get Collisions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4UFBJK0P--Mf"
      },
      "outputs": [],
      "source": [
        "timestamps_collisions = pd.read_excel(os.path.join(ROOTDIR_DATASET_NORMAL, \"20220811_collisions_timestamp.xlsx\"))\n",
        "timestamps_collisions['Timestamp'] = timestamps_collisions['Timestamp'] - pd.to_timedelta(2, 'h')\n",
        "# due to a time discrepancy, the time interval of the collisions should be anticipated of two hour\n",
        "start_col = timestamps_collisions[timestamps_collisions['Inizio/fine'] == \"i\"][['Timestamp']].rename(columns={'Timestamp': 'start'}) # even indexes\n",
        "end_col = timestamps_collisions[timestamps_collisions['Inizio/fine'] == \"f\"][['Timestamp']].rename(columns={'Timestamp': 'end'})     # odd indexes\n",
        "\n",
        "start_col.reset_index(drop=True, inplace=True)  # reset the indexes\n",
        "end_col.reset_index(drop=True, inplace=True)\n",
        "\n",
        "df_collision = pd.concat([start_col, end_col], axis=1)  # concatenate start e end --> it becomes (key, start, end) 51 columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o81Kdzf9P38y"
      },
      "source": [
        "# K-MEANS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ns_ehQr5HeKJ"
      },
      "outputs": [],
      "source": [
        "class KMeansAD(BaseEstimator, OutlierMixin):\n",
        "    def __init__(self, k: int, window_size: int, padding_length: int):\n",
        "        self.k = k                               # number of clusters\n",
        "        self.window_size = window_size           # window size\n",
        "        self.model = KMeans(n_clusters=k)        # model initialization\n",
        "        self.padding_length = padding_length     # padding (length to add at the end of our score array)\n",
        "\n",
        "    def _custom_reverse_windowing(self, scores: np.ndarray) -> np.ndarray:\n",
        "        print(\"Reversing window-based scores to point-based scores:\")\n",
        "        print(f\"Before reverse-windowing: scores.shape={scores.shape}\")\n",
        "        begins = np.arange(scores.shape[0]) * self.window_size                        # indexes of when window starts\n",
        "        ends = begins + self.window_size                                              # indexes of when window ends\n",
        "        unwindowed_length = scores.shape[0] * self.window_size + self.padding_length  # total length without windows\n",
        "        mapped = np.full(unwindowed_length, fill_value=np.nan)                        # [nan nan nan ... nan nan nan]\n",
        "        indices = np.unique(np.r_[begins, ends])                                      # [0 1 2 ... 34273 34274 34275], indexes of the scores\n",
        "        for i, j in zip(indices[:-1], indices[1:]):                                   # arrange tuple [1, 2] [2, 3] ...\n",
        "            if i >= 0 and j <= unwindowed_length:                                     # check if i is positive and j is lower than unwindowed_length\n",
        "                window_indices = np.flatnonzero((begins <= i) & (j <= ends))          # it gives us the indexes in the window\n",
        "                if len(window_indices) > 0:                                           # check if it's positive\n",
        "                    mapped[i:j] = np.nanmean(scores[window_indices])                  # map the value between i and j computing the mean\n",
        "        np.nan_to_num(mapped, copy = False)                                           # convert the nan values into number\n",
        "        print(f\"After reverse-windowing: scores.shape={mapped.shape}\")                # we have our mapped point-based scores\n",
        "        return mapped\n",
        "\n",
        "    def fit(self, X: np.ndarray, y=None, preprocess=True) -> 'KMeansAD':\n",
        "        self.model.fit(X)     # compute k-means clustering, y ignored\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: np.ndarray, preprocess=True) -> np.ndarray:\n",
        "        clusters = self.model.predict(X)    # predict cluster index for each sample\n",
        "        diffs = np.linalg.norm(X - self.model.cluster_centers_[clusters], axis=1)\n",
        "                # after assigning cluster labels, compute euclidean distance (L2 Norm) between every data point and the cluster center to whom they have been assigned to\n",
        "                # np.linalg.norm: compute euclidean norm\n",
        "                # axis = 1: specified that the norm must be calculated along axis = 1, which means that it is computed for each row (datapoint) individually\n",
        "                # diff: array where each element represent the distance between the point and the cluster\n",
        "                # these distances will be used to compute the anomaly scores. Furthers distances indicated that the point is far away from the cluster and could be considered as an anomaly\n",
        "\n",
        "        return self._custom_reverse_windowing(diffs)\n",
        "\n",
        "    def fit_predict(self, X, y=None) -> np.ndarray:\n",
        "        self.fit(X, y, preprocess=False)\n",
        "        return self.predict(X, preprocess=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPqWqI_7BGsy"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "def get_features(domain, df, frequency, window_size):\n",
        "    cfg = tsfel.get_features_by_domain(domain)                     # receive the features by domain: 20 statistical, 14 temporal, 26 spectral\n",
        "    extracted_features = tsfel.time_series_features_extractor(cfg,\n",
        "                                                              df.select_dtypes(['number']),\n",
        "                                                              fs=frequency,\n",
        "                                                              window_size=window_size,\n",
        "                                                              verbose=False)\n",
        "    return extracted_features     # return an array of shape [windows, features]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdzQjeSngv03"
      },
      "outputs": [],
      "source": [
        "def compute_anomaly_scores(data, df_size, n_clusters = 10, window_size = 10, padding_length = 0):\n",
        "\n",
        "  padding_length = df_size - (data.shape[0] * window_size)        # dimension you need to add\n",
        "\n",
        "  detector = KMeansAD(n_clusters, window_size, padding_length)    # init the model\n",
        "  start_time = time.time()\n",
        "  anomaly_scores = detector.fit_predict(data)                             # return the scores\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "  anomaly_scores = (anomaly_scores - np.min(anomaly_scores)) / (np.max(anomaly_scores) - np.min(anomaly_scores)) # Normalizing anomaly scores\n",
        "  return anomaly_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vklWKSxb_QiE"
      },
      "outputs": [],
      "source": [
        "def pre_processing(data):  # pre-processing without highly correlated features\n",
        "\n",
        "  data = data.drop((data.columns[data.isna().any()].tolist()), axis = 1) # remove nan values\n",
        "\n",
        "  scaler = preprocessing.StandardScaler() # Normalizing_features\n",
        "  scaler.fit(data)\n",
        "  data = pd.DataFrame(scaler.transform(data), columns=data.columns)\n",
        "\n",
        "  selector_variance = VarianceThreshold() # Remove zero-variance features\n",
        "  selector_variance.fit(data)\n",
        "  data = pd.DataFrame(selector_variance.transform(data), columns=data.columns.values[selector_variance.get_support()])\n",
        "\n",
        "  return data\n",
        "\n",
        "def features_extractions(window_size, n_clusters, domain):\n",
        "    # window_size_spectral = 15\n",
        "    # domain: # statistical, temporal, spectral\n",
        "    features = get_features(domain, X_collisions, frequency, window_size) # (windows, features) (3427, 2200)\n",
        "    features_norm = pre_processing(features)\n",
        "    return features_norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F5GoaGv1a7i"
      },
      "source": [
        "### Computing scores on test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDxQnU5Dqkqh"
      },
      "outputs": [],
      "source": [
        "features = features_extractions(window_size=10, n_clusters=15, domain='statistical')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HnUe76njK-1"
      },
      "outputs": [],
      "source": [
        "scores = compute_anomaly_scores(features, X_collisions.shape[0], n_clusters=15, window_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhSYvpyMiB5S"
      },
      "source": [
        "### Evaluate functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NRKDOt-LXXAX"
      },
      "outputs": [],
      "source": [
        "# plot distribution and return the true_labels\n",
        "def plot_hist(anomaly_scores, df_collision, df, title):\n",
        "    index_anomaly = []      # anomalies' index\n",
        "    idx = 0\n",
        "    for _, row in df.iterrows():\n",
        "        for _, collision_row in df_collision.iterrows():\n",
        "            if (row['time'] >= collision_row['start']) and (row['time'] <= collision_row['end']):\n",
        "                index_anomaly.append(idx)\n",
        "        idx += 1\n",
        "    true_labels = np.zeros_like(anomaly_scores)\n",
        "    true_labels[index_anomaly] = 1\n",
        "    logging.info(f\"Anomalies detected: {int(true_labels.sum())}\")\n",
        "    anomaly_values = anomaly_scores[index_anomaly]\n",
        "    normal_values = np.delete(anomaly_scores, index_anomaly)\n",
        "\n",
        "    plt.hist(normal_values, bins=30, color=\"tab:blue\", ec=\"dodgerblue\", alpha=0.5, label='Normal')\n",
        "    plt.hist(anomaly_values, bins=30, color='tab:red', ec=\"darkred\", alpha=0.7, label='Anomalies')\n",
        "\n",
        "    plt.xlabel('Values')\n",
        "    plt.ylabel('Occurrencies')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title(title)\n",
        "    plt.savefig(f'/content/{title}.jpg')  # Modify the path and filename as needed\n",
        "    plt.show()\n",
        "    return true_labels\n",
        "\n",
        "# compute f1, fB score, auc-roc, auc-pr\n",
        "def compute_metrics(anomaly_scores_norm, df_test, y_true, th=None):\n",
        "    tot_anomalies = y_true.sum()\n",
        "    sens = list()           # recalls o tpr\n",
        "    spec = list()\n",
        "    fpr = list()\n",
        "    f1 = list()\n",
        "    f0_1= list()\n",
        "    prec = list()\n",
        "    cm_list = list()\n",
        "    anomlay_indexes_dict = dict()\n",
        "    acc_with_err = list()\n",
        "    step = 0.01\n",
        "    ths = np.arange(0, 1, step)\n",
        "    if th is None:\n",
        "        for threshold in tqdm(ths):\n",
        "            anomalies_pred = anomaly_scores_norm > threshold\n",
        "            tp = 0                                                          # true positive per quella threshold\n",
        "            anomaly_indexes = list()\n",
        "            for index, anomaly_pred in enumerate(anomalies_pred):\n",
        "                if y_true[index] and anomaly_pred:\n",
        "                    anomaly_indexes.append(index)\n",
        "                    tp += 1\n",
        "\n",
        "            cm_anomaly = np.zeros((2,2))\n",
        "            n_sample = len(df_test)\n",
        "            n_not_collision = n_sample - tot_anomalies\n",
        "            n_detected = anomalies_pred.sum()\n",
        "\n",
        "            fp = n_detected - tp\n",
        "            fn = tot_anomalies - tp\n",
        "            tn = n_not_collision - fp\n",
        "\n",
        "            cm_anomaly[0, 0] = tn\n",
        "            cm_anomaly[0, 1] = fp\n",
        "            cm_anomaly[1, 0] = fn\n",
        "            cm_anomaly[1, 1] = tp\n",
        "\n",
        "            cm_list.append(cm_anomaly)\n",
        "            recall = tp / (tp + fn)\n",
        "            sens.append(recall)\n",
        "            fpr.append(1 - tn /(tn + fp))\n",
        "            precision = tp / (tp + fp)\n",
        "            prec.append(precision)\n",
        "            spec.append(tn /(tn + fp))\n",
        "            f1.append(2 * tp / (2 * tp + fp + fn))\n",
        "            f0_1.append((1 + 0.1**2) * tp / ((1 + 0.1**2) * tp +  0.1**2*fp + fn))\n",
        "            cm_anomaly_norm = cm_anomaly.astype('float') / cm_anomaly.sum(axis=1)[:, np.newaxis]\n",
        "            acc_with_err.append( (np.mean(np.diag(cm_anomaly_norm)), np.std(np.diag(cm_anomaly_norm))) )\n",
        "            anomlay_indexes_dict[threshold] = anomaly_indexes\n",
        "\n",
        "        f1_max = max(f1)\n",
        "        f0_1_max = max(f0_1)\n",
        "        max_index_f1 = f1.index(f1_max)\n",
        "        max_index_f0_1 = f0_1.index(f0_1_max)\n",
        "        th_f1_max = max_index_f1 * step\n",
        "        th_f0_1_max = max_index_f0_1 * step\n",
        "        print(f\"f1: {f1_max} at th: {th_f1_max}\")\n",
        "        print(f\"f0.1: {f0_1_max} at th: {th_f0_1_max}\")\n",
        "        print(f\"AUC-PR: {metrics.average_precision_score(y_true, anomaly_scores_norm)}\")\n",
        "        print(f\"AUC-ROC: {metrics.roc_auc_score(y_true, anomaly_scores_norm)}\")\n",
        "        return sens, fpr, th_f1_max\n",
        "    else:\n",
        "        df_anomaly = df_test.loc[np.array(anomaly_scores_norm > th)]\n",
        "        tp = 0                                                          # true positive per quella threshold\n",
        "        anomaly_indexes = list()\n",
        "        anomalies_pred = anomaly_scores_norm > th\n",
        "\n",
        "        for index, anomaly_pred in enumerate(anomalies_pred):\n",
        "            if y_true[index] and anomaly_pred:\n",
        "                anomaly_indexes.append(index)\n",
        "                tp += 1\n",
        "\n",
        "        cm_anomaly = np.zeros((2,2))\n",
        "        n_sample = len(df_test)\n",
        "        n_not_collision = n_sample - tot_anomalies\n",
        "        n_detected = len(df_anomaly)\n",
        "\n",
        "        fp = n_detected - tp\n",
        "        fn = tot_anomalies - tp\n",
        "        tn = n_not_collision - fp\n",
        "\n",
        "        cm_anomaly[0, 0] = tn\n",
        "        cm_anomaly[0, 1] = fp\n",
        "        cm_anomaly[1, 0] = fn\n",
        "        cm_anomaly[1, 1] = tp\n",
        "\n",
        "        f1 = 2 * tp / (2 * tp + fp + fn)\n",
        "        f0_1 = (1 + 0.1**2) * tp / ((1 + 0.1**2) * tp +  0.1**2*fp + fn)\n",
        "        print(f\"f1: {f1} at th: {th} for the test set\")\n",
        "        print(f\"f0.1: {f0_1} at th: {th} for the test set\")\n",
        "\n",
        "# another way to compute true_labels\n",
        "def create_true_labels(df_test, df_collision, scores):\n",
        "    index_anomaly = []\n",
        "    idx = 0\n",
        "    for _, row in df_test.iterrows():    # prende la riga da df_validation\n",
        "        for _, collision_row in df_collision.iterrows():  # prende la collision da df_collision\n",
        "            if (row['time'] >= collision_row['start']) and (row['time'] <= collision_row['end']):\n",
        "                index_anomaly.append(idx)         # salva l'indice\n",
        "        idx += 1               # aumenta l'indice\n",
        "    true_labels = np.zeros_like(scores)\n",
        "    true_labels[index_anomaly] = 1\n",
        "    logging.info(f\"Anomalies detected: {int(true_labels.sum())}\")\n",
        "    return true_labels\n",
        "\n",
        "# dataset divition for testing with validation\n",
        "def dataset_div(X_collisions, anomaly_scores_norm, df_test):\n",
        "  split = 0.9                                    # splitting value\n",
        "  split_at = int(len(X_collisions) * split)      # elements\n",
        "\n",
        "  asn_val = anomaly_scores_norm[split_at:]       # validation scores\n",
        "  asn_col = anomaly_scores_norm[:split_at]       # test scores\n",
        "\n",
        "  df_val = df_test.iloc[split_at:]\n",
        "  df_col = df_test.iloc[:split_at]\n",
        "\n",
        "  df_val = df_val[-asn_val.shape[0]:]\n",
        "  df_col = df_col[-asn_col.shape[0]:]\n",
        "\n",
        "  return df_val, df_col, asn_val, asn_col"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vslrXBR7iEqr"
      },
      "source": [
        "# Testing on a single model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuTRpyYFgMrr"
      },
      "source": [
        "##### Uploading scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "glgBpxWdu_rb"
      },
      "outputs": [],
      "source": [
        "with open('/content/kmeans_f10_clusters15_w10', \"rb\") as file:\n",
        "      scores = pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MQ2zqZ_gTLr"
      },
      "source": [
        "##### Computing true_labels and metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-tjzCnUib54"
      },
      "outputs": [],
      "source": [
        "true_labels = plot_hist(scores, df_collision, df_test, title='KMeans_distribution_f=10Hz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEJ1uXs5iEPf"
      },
      "outputs": [],
      "source": [
        "compute_metrics(scores, df_test, true_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQOYOO1lfoA3"
      },
      "source": [
        "##### Testing with validation split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4cPXb1ofqnp"
      },
      "outputs": [],
      "source": [
        "df_val, df_col, asn_val, asn_col = dataset_div(X_collisions, scores, df_test)\n",
        "true_labels_val = plot_hist(asn_val, df_collision, df_val, title='KMeans_Distribution_Val_f=10Hz')\n",
        "_, _, th_f1_max = compute_metrics(asn_val, df_val, true_labels_val)\n",
        "true_labels_test = plot_hist(asn_col, df_collision, df_col, title='KMeans_Distribution_Test_f=10Hz')\n",
        "compute_metrics(asn_col, df_col, true_labels_test, th_f1_max)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWna5T4Yfrc1"
      },
      "source": [
        "##### Downloading scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7F80Ub_fvZo"
      },
      "outputs": [],
      "source": [
        "anomaly_score = {\n",
        "            'anomaly_scores_norm' : scores,\n",
        "            'true_labels' : true_labels\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALG9e9V8cEBz"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/kmeans_f10_clusters15_w10.pkl', 'wb') as file:\n",
        "    pickle.dump(anomaly_score, file)\n",
        "files.download('/content/drive/MyDrive/kmeans_f10_clusters15_w10.pkl')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "d38q1TI3fGMP"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
