{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaZyI_wx2GCh"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNfzbXX42OLQ"
      },
      "source": [
        "##### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9woTAitF2L5V"
      },
      "outputs": [],
      "source": [
        "!pip install pandas==1.5.3\n",
        "!pip install tsfel\n",
        "!pip3 install --upgrade --no-cache-dir gdown       # support for download a large file from Google Drive\n",
        "!pip install numpy>=1.19.5\n",
        "!pip install scikit-learn>=0.24.1\n",
        "!pip install tadpak"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqvONELM2Rfc"
      },
      "source": [
        "##### Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pveDtgziiN52"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b_4_n1kim49"
      },
      "outputs": [],
      "source": [
        "# unzip from drive\n",
        "!unzip /content/drive/MyDrive/Colab_MLA/MLA_Project/csv_20220811.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJtsRSBP2Yil"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import time\n",
        "import tsfel\n",
        "import warnings\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import matplotlib.cm as cm\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics\n",
        "from google.colab import files\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, top_k_accuracy_score, f1_score, roc_curve, auc, precision_recall_curve\n",
        "from matplotlib.gridspec import GridSpec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrippWm52aTC"
      },
      "outputs": [],
      "source": [
        "ROOTDIR_DATASET_NORMAL = \"/content/csv_20220811\"\n",
        "plt.style.use(\"Solarize_Light2\") # Set style for matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2cbVS3Z2a3D"
      },
      "outputs": [],
      "source": [
        "def get_metadata(filepaths_csv, filepaths_meta, action2int=None, delimiter=\";\"):\n",
        "  dfs_meta = list()\n",
        "  for filepath in filepaths_meta:                                   # read filepath (0, 2, 3, 4)\n",
        "        df_m = pd.read_csv(filepath, sep=delimiter)                 # read csv files\n",
        "        df_m.str_repr = df_m.str_repr.str.replace('True', 'true')   # replace True with true\n",
        "        df_m['filepath'] = filepath                                 # create the 'filepath' column\n",
        "        dfs_meta.append(df_m)                                       # add the corresponding dataframe\n",
        "  df_meta = pd.concat(dfs_meta)                                     # concatenate all dataframes\n",
        "\n",
        "  df_meta.index = pd.to_datetime(df_meta.init_timestamp.astype('datetime64[ms]'), format=\"%Y-%m-%dT%H:%M:%S.%f\")                        # convert numerical index in time index\n",
        "  df_meta['completed_timestamp'] = pd.to_datetime(df_meta.completed_timestamp.astype('datetime64[ms]'), format=\"%Y-%m-%dT%H:%M:%S.%f\")  # change format of completed_timestamp\n",
        "  df_meta['init_timestamp'] = pd.to_datetime(df_meta.init_timestamp.astype('datetime64[ms]'), format=\"%Y-%m-%dT%H:%M:%S.%f\")            # change format of init_timestamp\n",
        "\n",
        "  actions = df_meta.str_repr.unique()                                             # due to the concat of before we know take the actions removing the duplicate\n",
        "\n",
        "  dfs = [pd.read_csv(filepath_csv, sep=\";\") for filepath_csv in filepaths_csv]    # read the train/test datas\n",
        "  df = pd.concat(dfs)                                                             # concat the data like before\n",
        "  df = df.sort_index(axis=1)                                                      # sort columns by name\n",
        "\n",
        "  df.index = pd.to_datetime(df.time.astype('datetime64[ms]'), format=\"%Y-%m-%dT%H:%M:%S.%f\")        # set timestamp as index\n",
        "  columns_to_drop = [column for column in df.columns if \"Abb\" in column or \"Temperature\" in column] # remove uselesse columns\n",
        "  df.drop([\"machine_nameKuka Robot_export_active_energy\", \"machine_nameKuka Robot_import_reactive_energy\"] + columns_to_drop, axis=1, inplace=True)\n",
        "\n",
        "  df_action = list()        # take the actions\n",
        "  for action in actions:    # loop for each action (30)\n",
        "      for index, row in df_meta[df_meta.str_repr == action].iterrows(): # get index and row from metadata where the actions are the same\n",
        "          start = row['init_timestamp']         # start\n",
        "          end = row['completed_timestamp']      # end\n",
        "          df_tmp = df.loc[start: end].copy()    # temporary dataframe\n",
        "          df_tmp['action'] = action             # get action\n",
        "          df_tmp['duration'] = str((row['completed_timestamp'] - row['init_timestamp']).total_seconds())    # life of the action (it's not a feature)\n",
        "          df_action.append(df_tmp)\n",
        "  df_action = pd.concat(df_action, ignore_index=True)     # concatenate the actions\n",
        "  df_action.index = pd.to_datetime(df_action.time.astype('datetime64[ms]'), format=\"%Y-%m-%dT%H:%M:%S.%f\")   # set the time as index\n",
        "  df_action = df_action[~df_action.index.duplicated(keep='first')]     # keep the duplicate\n",
        "\n",
        "  df = df.dropna(axis=0)                  # remove NaNs from Df (34275, 56)\n",
        "  df_action = df_action.dropna(axis=0)    # (33063, 58)\n",
        "\n",
        "  if action2int is None:        # map the actions to integer --> 30 action - 30 indexes from 1 to 30\n",
        "      action2int = dict()\n",
        "      j = 1\n",
        "      for label in df_action.action.unique():\n",
        "          action2int[label] = j\n",
        "          j += 1\n",
        "\n",
        "  df_merged = df.merge(df_action[['action']], left_index=True, right_index=True, how=\"left\")  # (34275, 57)\n",
        "  df_idle = df_merged[df_merged['action'].isna()].copy()    # (1212, 57)\n",
        "  df_idle['action'] = 'idle'\n",
        "  df_idle['duration'] = df_action.duration.values.astype(float).mean().astype(str)\n",
        "  df_action = pd.concat([df_action, df_idle]) # (34275, 58)\n",
        "  action2int['idle'] = 0\n",
        "  return df_action, df, df_meta, action2int"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_smUIdnQ2g7m"
      },
      "source": [
        "##### Get dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBEvGI3E2fLz"
      },
      "outputs": [],
      "source": [
        "frequency = 1    # 1 10 100 200 Hz - life {1: 10, 10: 1, 100: 0.1, 200: 0.05}\n",
        "\n",
        "filepath_csv_test = [os.path.join(ROOTDIR_DATASET_NORMAL, f\"rec{r}_collision_20220811_rbtc_{1/frequency}s.csv\") for r in [1, 5]]        # read data with anomalies\n",
        "filepath_meta_test = [os.path.join(ROOTDIR_DATASET_NORMAL, f\"rec{r}_collision_20220811_rbtc_{1/frequency}s.metadata\") for r in[1, 5]]\n",
        "\n",
        "filepath_csv_train = [os.path.join(ROOTDIR_DATASET_NORMAL, f\"rec{r}_20220811_rbtc_{1/frequency}s.csv\") for r in [0, 2, 3, 4]]           # read non-anomalous data\n",
        "filepath_meta_train = [os.path.join(ROOTDIR_DATASET_NORMAL, f\"rec{r}_20220811_rbtc_{1/frequency}s.metadata\") for r in [0, 2, 3, 4]]\n",
        "\n",
        "df_action_train, df_train, df_meta_train, action2int_train = get_metadata(filepath_csv_train, filepath_meta_train)    # read corresponding metadata\n",
        "df_action_test, df_test, df_meta_test, action2int_test = get_metadata(filepath_csv_test, filepath_meta_test)\n",
        "\n",
        "df_test['time'] = pd.to_datetime(df_test.time.astype('datetime64[ms]'), format =\"%Y-%m-%dT%H:%M:%S.%f\")\n",
        "\n",
        "X_train = df_train.drop(['time'], axis=1, inplace=False)     # remove last column 'time' from dataset\n",
        "X_collisions = df_test.drop(['time'], axis=1, inplace=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqmGzo562ie-"
      },
      "source": [
        "##### Get collisions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00UH-Is-2gUC"
      },
      "outputs": [],
      "source": [
        "timestamps_collisions = pd.read_excel(os.path.join(ROOTDIR_DATASET_NORMAL, \"20220811_collisions_timestamp.xlsx\"))\n",
        "timestamps_collisions['Timestamp'] = timestamps_collisions['Timestamp'] - pd.to_timedelta(2, 'h')\n",
        "# due to a time discrepancy, the time interval of the collisions should be anticipated of two hour\n",
        "start_col = timestamps_collisions[timestamps_collisions['Inizio/fine'] == \"i\"][['Timestamp']].rename(columns={'Timestamp': 'start'}) # even indexes\n",
        "end_col = timestamps_collisions[timestamps_collisions['Inizio/fine'] == \"f\"][['Timestamp']].rename(columns={'Timestamp': 'end'})     # odd indexes\n",
        "\n",
        "start_col.reset_index(drop=True, inplace=True)  # reset the indexes\n",
        "end_col.reset_index(drop=True, inplace=True)\n",
        "\n",
        "df_collision = pd.concat([start_col, end_col], axis=1)  # concatenate start e end --> it becomes (key, start, end) 51 columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaPwZgIO2pm3"
      },
      "source": [
        "### Plotting functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1ZvSpZc2s2U"
      },
      "outputs": [],
      "source": [
        "# plotting a small samples of scores\n",
        "def plotting_anomaly(scores, title, samples=2500):\n",
        "  data = scores[315:samples]\n",
        "  x = np.arange(1000, len(data) + 1000)\n",
        "  plt.plot(x, data, color='salmon', label='Anomaly Scores')\n",
        "  plt.xlabel('Point')\n",
        "  plt.ylabel('Score')\n",
        "  plt.title(title)\n",
        "  plt.savefig(f'/content/{title}.jpg')\n",
        "  plt.show()\n",
        "\n",
        "# plot distribution and return the true_labels\n",
        "def plot_hist(anomaly_scores, df_collision, df, title):\n",
        "    index_anomaly = []      # anomalies' index\n",
        "    idx = 0\n",
        "    for _, row in df.iterrows():\n",
        "        for _, collision_row in df_collision.iterrows():\n",
        "            if (row['time'] >= collision_row['start']) and (row['time'] <= collision_row['end']):\n",
        "                index_anomaly.append(idx)\n",
        "        idx += 1\n",
        "    true_labels = np.zeros_like(anomaly_scores)\n",
        "    true_labels[index_anomaly] = 1\n",
        "    logging.info(f\"Anomalies detected: {int(true_labels.sum())}\")\n",
        "    anomaly_values = anomaly_scores[index_anomaly]\n",
        "    normal_values = np.delete(anomaly_scores, index_anomaly)\n",
        "\n",
        "    plt.hist(normal_values, bins=30, color=\"tab:blue\", ec=\"dodgerblue\", alpha=0.5, label='Normal')\n",
        "    plt.hist(anomaly_values, bins=30, color='tab:red', ec=\"darkred\", alpha=0.7, label='Anomalies')\n",
        "\n",
        "    plt.xlabel('Values')\n",
        "    plt.ylabel('Occurrencies')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title(title)\n",
        "    plt.savefig(f'/content/{title}.jpg')  # Modify the path and filename as needed\n",
        "    plt.show()\n",
        "    return true_labels\n",
        "\n",
        "# dataset divition for testing with validation\n",
        "def dataset_div(X_collisions, anomaly_scores_norm, df_test):\n",
        "  split = 0.9                                    # splitting value\n",
        "  split_at = int(len(X_collisions) * split)      # elements\n",
        "\n",
        "  asn_val = anomaly_scores_norm[split_at:]       # validation scores\n",
        "  asn_col = anomaly_scores_norm[:split_at]       # test scores\n",
        "\n",
        "  df_val = df_test.iloc[split_at:]\n",
        "  df_col = df_test.iloc[:split_at]\n",
        "\n",
        "  df_val = df_val[-asn_val.shape[0]:]\n",
        "  df_col = df_col[-asn_col.shape[0]:]\n",
        "\n",
        "  return df_val, df_col, asn_val, asn_col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlvZr4ll2xqh"
      },
      "outputs": [],
      "source": [
        "# plotting a prediction segment\n",
        "def plot_prediction(df_test, y_true, anomaly_scores_norm, X_collisions, threshold, title, samples=1815):\n",
        "    df_test = df_test[315:samples]              # selecting samples\n",
        "    X_collisions = X_collisions[315:samples]\n",
        "    y_true = y_true[315:samples]                # selecting labels\n",
        "    anomaly_scores_norm = anomaly_scores_norm[315:samples] # selecting scores\n",
        "\n",
        "    pca = PCA(n_components=1)\n",
        "    X_collisions_pca = pca.fit_transform(X_collisions)\n",
        "\n",
        "    X_collisions_pca = X_collisions['machine_nameKuka Robot_apparent_power']\n",
        "    print(f\"{X_collisions_pca.shape=}\")\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 6))\n",
        "    gs = GridSpec(2, 1, height_ratios=[2, 1])\n",
        "\n",
        "    # Plot della serie temporale (sopra)\n",
        "    ax1 = fig.add_subplot(gs[0])\n",
        "    ax1.plot(df_test['time'], X_collisions_pca, label='Serie Temporale', color='darkgreen', zorder=1)  # Imposta zorder\n",
        "\n",
        "    ax1.set_ylabel('')\n",
        "    ax1.yaxis.set_visible(False)\n",
        "\n",
        "    # Imposta il colore di bordo a 'none' per rimuovere il riquadro\n",
        "    ax1.spines['top'].set_visible(False)\n",
        "    ax1.spines['right'].set_visible(False)\n",
        "    ax1.spines['bottom'].set_visible(False)\n",
        "    ax1.spines['left'].set_visible(False)\n",
        "    # Rimuovi le spine in basso impostando la loro visibilitÃ  a False\n",
        "    ax1.xaxis.set_ticks_position('none')\n",
        "    ax1.yaxis.set_ticks_position('none')\n",
        "\n",
        "    condition_red = anomaly_scores_norm > threshold\n",
        "    condition_orange = y_true\n",
        "\n",
        "    ax1.fill_between(df_test['time'], X_collisions_pca, where=condition_red, color='red', alpha=0.5, zorder=2)\n",
        "    ax1.fill_between(df_test['time'], X_collisions_pca, where=condition_orange, color='blue', alpha=0.3, zorder=3)\n",
        "\n",
        "    # Plot dello score (sotto)\n",
        "    ax2 = fig.add_subplot(gs[1], sharex=ax1)\n",
        "    ax2.plot(df_test['time'], anomaly_scores_norm, label='Score', color='orange')\n",
        "    ax2.set_ylabel('Score')\n",
        "    ax2.axhline(y=threshold, color='red', linestyle='--', label=f'Threshold ({threshold})')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Timestamp')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'/content/{title}.jpg')  # Modify the path and filename as needed\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-lkQi5E2zSi"
      },
      "source": [
        "### Evaluate functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXdfQ3-n2zXR"
      },
      "outputs": [],
      "source": [
        "# compute f1, fB score, auc-roc, auc-pr\n",
        "def compute_metrics(anomaly_scores_norm, df_test, y_true, th=None):\n",
        "    tot_anomalies = y_true.sum()\n",
        "    sens = list()           # recalls o tpr\n",
        "    spec = list()\n",
        "    fpr = list()\n",
        "    f1 = list()\n",
        "    f0_1= list()\n",
        "    prec = list()\n",
        "    cm_list = list()\n",
        "    anomlay_indexes_dict = dict()\n",
        "    acc_with_err = list()\n",
        "    step = 0.01\n",
        "    ths = np.arange(0, 1, step)\n",
        "    if th is None:\n",
        "        for threshold in tqdm(ths):\n",
        "            anomalies_pred = anomaly_scores_norm > threshold\n",
        "            tp = 0                                                          # true positive per quella threshold\n",
        "            anomaly_indexes = list()\n",
        "            for index, anomaly_pred in enumerate(anomalies_pred):\n",
        "                if y_true[index] and anomaly_pred:\n",
        "                    anomaly_indexes.append(index)\n",
        "                    tp += 1\n",
        "\n",
        "            cm_anomaly = np.zeros((2,2))\n",
        "            n_sample = len(df_test)\n",
        "            n_not_collision = n_sample - tot_anomalies\n",
        "            n_detected = anomalies_pred.sum()\n",
        "\n",
        "            fp = n_detected - tp\n",
        "            fn = tot_anomalies - tp\n",
        "            tn = n_not_collision - fp\n",
        "\n",
        "            cm_anomaly[0, 0] = tn\n",
        "            cm_anomaly[0, 1] = fp\n",
        "            cm_anomaly[1, 0] = fn\n",
        "            cm_anomaly[1, 1] = tp\n",
        "\n",
        "            cm_list.append(cm_anomaly)\n",
        "            recall = tp / (tp + fn)\n",
        "            sens.append(recall)\n",
        "            fpr.append(1 - tn /(tn + fp))\n",
        "            precision = tp / (tp + fp)\n",
        "            prec.append(precision)\n",
        "            spec.append(tn /(tn + fp))\n",
        "            f1.append(2 * tp / (2 * tp + fp + fn))\n",
        "            f0_1.append((1 + 0.1**2) * tp / ((1 + 0.1**2) * tp +  0.1**2*fp + fn))\n",
        "            cm_anomaly_norm = cm_anomaly.astype('float') / cm_anomaly.sum(axis=1)[:, np.newaxis]\n",
        "            acc_with_err.append( (np.mean(np.diag(cm_anomaly_norm)), np.std(np.diag(cm_anomaly_norm))) )\n",
        "            anomlay_indexes_dict[threshold] = anomaly_indexes\n",
        "\n",
        "        f1_max = max(f1)\n",
        "        f0_1_max = max(f0_1)\n",
        "        max_index_f1 = f1.index(f1_max)\n",
        "        max_index_f0_1 = f0_1.index(f0_1_max)\n",
        "        th_f1_max = max_index_f1 * step\n",
        "        th_f0_1_max = max_index_f0_1 * step\n",
        "        print(f\"f1: {f1_max} at th: {th_f1_max}\")\n",
        "        print(f\"f0.1: {f0_1_max} at th: {th_f0_1_max}\")\n",
        "        print(f\"AUC-PR: {metrics.average_precision_score(y_true, anomaly_scores_norm)}\")\n",
        "        print(f\"AUC-ROC: {metrics.roc_auc_score(y_true, anomaly_scores_norm)}\")\n",
        "        return sens, fpr, th_f1_max\n",
        "    else:\n",
        "        df_anomaly = df_test.loc[np.array(anomaly_scores_norm > th)]\n",
        "        tp = 0                                                          # true positive per quella threshold\n",
        "        anomaly_indexes = list()\n",
        "        anomalies_pred = anomaly_scores_norm > th\n",
        "\n",
        "        for index, anomaly_pred in enumerate(anomalies_pred):\n",
        "            if y_true[index] and anomaly_pred:\n",
        "                anomaly_indexes.append(index)\n",
        "                tp += 1\n",
        "\n",
        "        cm_anomaly = np.zeros((2,2))\n",
        "        n_sample = len(df_test)\n",
        "        n_not_collision = n_sample - tot_anomalies\n",
        "        n_detected = len(df_anomaly)\n",
        "\n",
        "        fp = n_detected - tp\n",
        "        fn = tot_anomalies - tp\n",
        "        tn = n_not_collision - fp\n",
        "\n",
        "        cm_anomaly[0, 0] = tn\n",
        "        cm_anomaly[0, 1] = fp\n",
        "        cm_anomaly[1, 0] = fn\n",
        "        cm_anomaly[1, 1] = tp\n",
        "\n",
        "        f1 = 2 * tp / (2 * tp + fp + fn)\n",
        "        f0_1 = (1 + 0.1**2) * tp / ((1 + 0.1**2) * tp +  0.1**2*fp + fn)\n",
        "        print(f\"f1: {f1} at th: {th} for the test set\")\n",
        "        print(f\"f0.1: {f0_1} at th: {th} for the test set\")\n",
        "\n",
        "# another way to compute true_labels\n",
        "def create_true_labels(df_test, df_collision, scores):\n",
        "    index_anomaly = []\n",
        "    idx = 0\n",
        "    for _, row in df_test.iterrows():    # prende la riga da df_validation\n",
        "        for _, collision_row in df_collision.iterrows():  # prende la collision da df_collision\n",
        "            if (row['time'] >= collision_row['start']) and (row['time'] <= collision_row['end']):\n",
        "                index_anomaly.append(idx)         # salva l'indice\n",
        "        idx += 1               # aumenta l'indice\n",
        "    true_labels = np.zeros_like(scores)\n",
        "    true_labels[index_anomaly] = 1\n",
        "    logging.info(f\"Anomalies detected: {int(true_labels.sum())}\")\n",
        "    return true_labels\n",
        "\n",
        "# auc_roc and auc_pr\n",
        "def roc_pr(true_labels, anomaly_scores_norm):\n",
        "  auc_roc = roc_auc_score(true_labels, anomaly_scores_norm)          # Compute AUC-ROC\n",
        "  auc_pr = average_precision_score(true_labels, anomaly_scores_norm) # Compute AUC-PR\n",
        "  print(f'AUC-ROC: {auc_roc:.2f}')\n",
        "  print(f'AUC-PR: {auc_pr:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mnfo-wC26cd"
      },
      "source": [
        "### Testing on a single score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFzAdI_s28pF"
      },
      "outputs": [],
      "source": [
        "with open('/content/kmeans_f10_clusters15_w10', \"rb\") as file:\n",
        "      scores = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKj6B_bj77Eu"
      },
      "outputs": [],
      "source": [
        "true_labels = plot_hist(scores, df_collision, df_test, title='KMeans_distribution_f=10Hz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzt0mCbb77_D"
      },
      "outputs": [],
      "source": [
        "compute_metrics(scores, df_test, true_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZAP-Sy428yw"
      },
      "source": [
        "### Testing on multiple scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFLyR5wn3Dbd"
      },
      "outputs": [],
      "source": [
        "with open('/content/enc_dec_f10.pickle', \"rb\") as file:\n",
        "      scores_encdec = pickle.load(file)\n",
        "with open('/content/lstm_f10.pickle', \"rb\") as file:\n",
        "      scores_lstm = pickle.load(file)\n",
        "with open('/content/hif_supervised_f10_trees1024_samples256.pkl', \"rb\") as file:\n",
        "      scores_hif_supervised = pickle.load(file)\n",
        "with open('/content/hif_unsupervised_f10_trees1024_samples_256.pkl', \"rb\") as file:\n",
        "      scores_hif_unsupervised = pickle.load(file)\n",
        "with open('/content/kmeans_f10_clusters15_w10.pkl', \"rb\") as file:\n",
        "      scores_kmeans = pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIGRQiuw3FZp"
      },
      "source": [
        "##### Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKWVaqs43GuP"
      },
      "outputs": [],
      "source": [
        "_ = plot_hist(scores_hif_supervised['anomaly_scores_norm'], df_collision, df_test[-scores_hif_supervised['anomaly_scores_norm'].shape[0]:], title='HIF_supervised_Distribution')\n",
        "_ = plot_hist(scores_encdec['anomaly_scores_norm'], df_collision, df_test[-scores_encdec['anomaly_scores_norm'].shape[0]:], title='EncDec-AD_Distribution')\n",
        "_ = plot_hist(scores_kmeans['anomaly_scores_norm'], df_collision, df_test, title='KMeans_Distribution')\n",
        "_ = plot_hist(scores_lstm['anomaly_scores_norm'], df_collision, df_test[-scores_lstm['anomaly_scores_norm'].shape[0]:], title='LSTM-AD_Distribution')\n",
        "_ = plot_hist(scores_hif_unsupervised['anomaly_scores_norm'], df_collision, df_test, title='HIF_unsupervised_Distribution')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl_cvgQY3Idn"
      },
      "source": [
        "##### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ear9RPxQ3H8n"
      },
      "outputs": [],
      "source": [
        "compute_metrics(scores_hif_supervised['anomaly_scores_norm'], df_test[-scores_hif_supervised['anomaly_scores_norm'].shape[0]:], scores_hif_supervised['true_labels'])\n",
        "compute_metrics(scores_encdec['anomaly_scores_norm'], df_test[-scores_encdec['anomaly_scores_norm'].shape[0]:], scores_encdec['true_labels'])\n",
        "compute_metrics(scores_kmeans['anomaly_scores_norm'], df_test, scores_kmeans['true_labels'])\n",
        "compute_metrics(scores_lstm['anomaly_scores_norm'], df_test[-scores_lstm['anomaly_scores_norm'].shape[0]:], scores_lstm['true_labels'])\n",
        "compute_metrics(scores_hif_unsupervised['anomaly_scores_norm'], df_test, scores_hif_unsupervised['true_labels'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHXReVAZ3Lyz"
      },
      "source": [
        "##### Scores plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_gFjFVM3L53"
      },
      "outputs": [],
      "source": [
        "plotting_anomaly(scores_encdec['anomaly_scores_norm'][-scores_hif_supervised['anomaly_scores_norm'].shape[0]:], title='Anomaly_scores_EncDec-AD')    # plotting of scores\n",
        "plotting_anomaly(scores_lstm['anomaly_scores_norm'][-scores_hif_supervised['anomaly_scores_norm'].shape[0]:], title='Anomaly_scores_LSTM-AD')    # plotting of scores\n",
        "plotting_anomaly(scores_hif_supervised['anomaly_scores_norm'], title='Anomaly_scores_HIF_supervised')    # plotting of scores\n",
        "plotting_anomaly(scores_hif_unsupervised['anomaly_scores_norm'][-scores_hif_supervised['anomaly_scores_norm'].shape[0]:], title='Anomaly_scores_HIF_unsupervised')    # plotting of scores\n",
        "plotting_anomaly(scores_kmeans['anomaly_scores_norm'][-scores_hif_supervised['anomaly_scores_norm'].shape[0]:], title='Anomaly_scores_KMeans')    # plotting of scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1PhlBZR3OPD"
      },
      "source": [
        "##### Prediction segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBvmn5C23OTz"
      },
      "outputs": [],
      "source": [
        "plot_prediction(df_test[-scores_hif_supervised['anomaly_scores_norm'].shape[0]:],\n",
        "                scores_encdec['true_labels'][-scores_hif_supervised['true_labels'].shape[0]:],\n",
        "                scores_encdec['anomaly_scores_norm'][-scores_hif_supervised['anomaly_scores_norm'].shape[0]:],\n",
        "                X_collisions[-scores_hif_supervised['anomaly_scores_norm'].shape[0]:],\n",
        "                threshold=0.32,\n",
        "                title='EncDec-AD_segment_prediction')\n",
        "plot_prediction(df_test[-scores_hif_supervised['anomaly_scores_norm'].shape[0]:],\n",
        "                scores_lstm['true_labels'][-scores_hif_supervised['true_labels'].shape[0]:],\n",
        "                scores_lstm['anomaly_scores_norm'][-scores_hif_supervised['anomaly_scores_norm'].shape[0]:],\n",
        "                X_collisions[-scores_hif_supervised['anomaly_scores_norm'].shape[0]:],\n",
        "                threshold=0.07,\n",
        "                title='LSTM-AD_segment_prediction')\n",
        "plot_prediction(df_test[-scores_hif_supervised['anomaly_scores_norm'].shape[0]:],\n",
        "                scores_kmeans['true_labels'][-scores_hif_supervised['true_labels'].shape[0]:],\n",
        "                scores_kmeans['anomaly_scores_norm'][-scores_hif_supervised['anomaly_scores_norm'].shape[0]:],\n",
        "                X_collisions[-scores_hif_supervised['anomaly_scores_norm'].shape[0]:],\n",
        "                threshold=0.31,\n",
        "                title='KMeans_segment_prediction')\n",
        "plot_prediction(df_test[-scores_hif_supervised['anomaly_scores_norm'].shape[0]:],\n",
        "                scores_hif_unsupervised['true_labels'][-scores_hif_supervised['true_labels'].shape[0]:],\n",
        "                scores_hif_unsupervised['anomaly_scores_norm'][-scores_hif_supervised['anomaly_scores_norm'].shape[0]:],\n",
        "                X_collisions[-scores_hif_supervised['anomaly_scores_norm'].shape[0]:],\n",
        "                threshold=0.63,\n",
        "                title='HIF_unsupervised_segment_prediction')\n",
        "plot_prediction(df_test[-scores_hif_supervised['anomaly_scores_norm'].shape[0]:],\n",
        "                scores_hif_supervised['true_labels'],\n",
        "                scores_hif_supervised['anomaly_scores_norm'],\n",
        "                X_collisions[-scores_hif_supervised['anomaly_scores_norm'].shape[0]:],\n",
        "                threshold=0.56,\n",
        "                title='HIF_supervised_segment_prediction')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pptj2jbI3UfM"
      },
      "source": [
        "##### AUC-ROC and AUC-PR plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb6jhf1T3VaU"
      },
      "outputs": [],
      "source": [
        "# Calculate ROC curve and AUC for each model\n",
        "fpr1, tpr1, _ = roc_curve(scores_hif_supervised['true_labels'], scores_hif_supervised['anomaly_scores_norm'])\n",
        "roc_auc1 = auc(fpr1, tpr1)\n",
        "\n",
        "fpr2, tpr2, _ = roc_curve(scores_lstm['true_labels'], scores_lstm['anomaly_scores_norm'])\n",
        "roc_auc2 = auc(fpr2, tpr2)\n",
        "\n",
        "fpr3, tpr3, _ = roc_curve(scores_encdec['true_labels'], scores_encdec['anomaly_scores_norm'])\n",
        "roc_auc3 = auc(fpr3, tpr3)\n",
        "\n",
        "fpr4, tpr4, _ = roc_curve(scores_kmeans['true_labels'], scores_kmeans['anomaly_scores_norm'])\n",
        "roc_auc4 = auc(fpr4, tpr4)\n",
        "\n",
        "# Calculate PR curve and AUC for each model\n",
        "precision1, recall1, _ = precision_recall_curve(scores_hif_supervised['true_labels'], scores_hif_supervised['anomaly_scores_norm'])\n",
        "pr_auc1 = average_precision_score(scores_hif_supervised['true_labels'], scores_hif_supervised['anomaly_scores_norm'])\n",
        "\n",
        "precision2, recall2, _ = precision_recall_curve(scores_lstm['true_labels'],  scores_lstm['anomaly_scores_norm'])\n",
        "pr_auc2 = average_precision_score(scores_lstm['true_labels'],  scores_lstm['anomaly_scores_norm'])\n",
        "\n",
        "precision3, recall3, _ = precision_recall_curve(scores_encdec['true_labels'], scores_encdec['anomaly_scores_norm'])\n",
        "pr_auc3 = average_precision_score(scores_encdec['true_labels'], scores_encdec['anomaly_scores_norm'])\n",
        "\n",
        "precision4, recall4, _ = precision_recall_curve(scores_kmeans['true_labels'], scores_kmeans['anomaly_scores_norm'])\n",
        "pr_auc4 = average_precision_score(scores_kmeans['true_labels'], scores_kmeans['anomaly_scores_norm'])\n",
        "\n",
        "# Plot ROC curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(fpr1, tpr1, label=f'HIF_supervised (AUC = {roc_auc1:.2f})')\n",
        "plt.plot(fpr2, tpr2, label=f'LSTM-AD (AUC = {roc_auc2:.2f})')\n",
        "plt.plot(fpr3, tpr3, label=f'EncDec-AD (AUC = {roc_auc3:.2f})')\n",
        "plt.plot(fpr4, tpr4, label=f'KMeans (AUC = {roc_auc4:.2f})')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves')\n",
        "plt.legend()\n",
        "plt.savefig('/content/auc_roc_curves.jpg')  # Modify the path and filename as needed\n",
        "plt.show()\n",
        "\n",
        "# Plot PR curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(recall1, precision1, label=f'HIF_supervised (AUC = {pr_auc1:.2f})')\n",
        "plt.plot(recall2, precision2, label=f'LSTM-AD (AUC = {pr_auc2:.2f})')\n",
        "plt.plot(recall3, precision3, label=f'EncDec-AD (AUC = {pr_auc3:.2f})')\n",
        "plt.plot(recall4, precision4, label=f'KMeans (AUC = {pr_auc4:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curves')\n",
        "plt.legend()\n",
        "plt.xlim([0.1, 1.0])\n",
        "plt.savefig('/content/auc_pr_curves.jpg')  # Modify the path and filename as needed\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDFzL9iV3Uju"
      },
      "source": [
        "### Precision@K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIpMOOU43XLI"
      },
      "outputs": [],
      "source": [
        "# Precision@k: All previous measures require an anomaly score threshold to be computed. An alternative approach is to measure\n",
        "# the Precision using a subset of anomalies corresponding to the ð‘˜ highest value in the anomaly score. This is equivalent to setting\n",
        "# the threshold such that only the k highest values are retrieved\n",
        "\n",
        "def precision_at_k(anomaly_scores, true_labels, k):\n",
        "    num_anomalies  = int(np.sum(true_labels))                                # number of anomalies scores\n",
        "    total_samples  = len(true_labels)\n",
        "    threshold = np.percentile(anomaly_scores, 100 * (1 - k/total_samples ))  # compute a dynamic threshold based on the k proportion of anomalies. This threshold is determined by the percentile of the anomaly scores\n",
        "    detected_anomalies  = np.where(anomaly_scores > threshold)[0]            # get the indeces of the samples where the anomaly scores exceed the dynamic threshold\n",
        "    true_positives = sum(true_labels[detected_anomalies ])                   # compute the number of TP among the top-k predictions\n",
        "    precision_at_k = true_positives / num_anomalies                          # compute precision at the. P = TP/(FP + TP) --> FP = total - TP\n",
        "    return precision_at_k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t3J_CMw3Zzd"
      },
      "source": [
        "##### Precision@K plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOS85nN23Z3E"
      },
      "outputs": [],
      "source": [
        "# K must be equal to a fraction of anomalies\n",
        "N = 90\n",
        "precisions_K_kmeans = []\n",
        "precisions_K_hif_supervised = []\n",
        "precisions_K_lstm = []\n",
        "precisions_K_encdec = []\n",
        "precisions_K_hif_unsupervised = []\n",
        "\n",
        "k_values = np.arange(1, N)\n",
        "\n",
        "for i in k_values: # from 1 % to 10% of anomalies\n",
        "  k = int((1736 * i) / 100) # 1736 is the total number of anomalies with frequency = 10Hz\n",
        "  precision_k_kmeans = precision_at_k(\n",
        "                                scores_kmeans['anomaly_scores_norm'],\n",
        "                                scores_kmeans['true_labels'], k)\n",
        "  precision_k_hif_supervised = precision_at_k(\n",
        "                                scores_hif_supervised['anomaly_scores_norm'],\n",
        "                                scores_hif_supervised['true_labels'], k)\n",
        "  precision_k_lstm = precision_at_k(\n",
        "                                scores_lstm['anomaly_scores_norm'],\n",
        "                                scores_lstm['true_labels'], k)\n",
        "  precision_k_encdec = precision_at_k(\n",
        "                                scores_encdec['anomaly_scores_norm'],\n",
        "                                scores_encdec['true_labels'], k)\n",
        "  precision_k_hif_unsupervised = precision_at_k(\n",
        "                                scores_hif_unsupervised['anomaly_scores_norm'],\n",
        "                                scores_hif_unsupervised['true_labels'], k)\n",
        "\n",
        "  precisions_K_kmeans.append(precision_k_kmeans)\n",
        "  precisions_K_hif_supervised.append(precision_k_hif_supervised)\n",
        "  precisions_K_lstm.append(precision_k_lstm)\n",
        "  precisions_K_encdec.append(precision_k_encdec)\n",
        "  precisions_K_hif_unsupervised.append(precision_k_hif_unsupervised)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_values, precisions_K_kmeans, label='KMeans')\n",
        "plt.plot(k_values, precisions_K_hif_supervised, label='HIF supervised')\n",
        "plt.plot(k_values, precisions_K_hif_unsupervised, label='HIF unsupervised')\n",
        "plt.plot(k_values, precisions_K_lstm, label='LSTM-AD')\n",
        "plt.plot(k_values, precisions_K_encdec, label='EncDec-AD')\n",
        "plt.xlabel('K % of anomalies')\n",
        "plt.ylabel('Precision@K')\n",
        "plt.title('Precision@K scores over different K values')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('/content/precision_at_K_scores_over_different_K_values.jpg')  # Modify the path and filename as needed\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_fNBDvZ3dEl"
      },
      "source": [
        "### PA@K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7X7ddYF3ehF"
      },
      "outputs": [],
      "source": [
        "def pak(scores, targets, thres, k=20):\n",
        "    \"\"\"\n",
        "\n",
        "    :param scores: anomaly scores\n",
        "    :param targets: target labels\n",
        "    :param thres: anomaly threshold\n",
        "    :param k: PA%K ratio, 0 equals to conventional point adjust and 100 equals to original predictions\n",
        "    :return: point_adjusted predictions\n",
        "    \"\"\"\n",
        "    scores = np.array(scores)     # convert anomaly scores and threholsd to in numpy array\n",
        "    thres = np.array(thres)\n",
        "\n",
        "    predicts = scores > thres     # each element is true if the score is greater than the threshold\n",
        "    actuals = targets > 0.01      # each elment is true if the corrisponding target label is greather than 0.01\n",
        "\n",
        "    one_start_idx = np.where(np.diff(actuals, prepend=0) == 1)[0]   # dentify the starting indices of consecutive sequences of 1s (one_start_idx) and 0s (zero_start_idx) in the actuals array.\n",
        "    zero_start_idx = np.where(np.diff(actuals, prepend=0) == -1)[0]\n",
        "\n",
        "    # If the length of one_start_idx is equal to the length of zero_start_idx + 1, adjust zero_start_idx by appending the length of predicts.\n",
        "    assert len(one_start_idx) == len(zero_start_idx) + 1 or len(one_start_idx) == len(zero_start_idx)\n",
        "\n",
        "    if len(one_start_idx) == len(zero_start_idx) + 1:\n",
        "        zero_start_idx = np.append(zero_start_idx, len(predicts))\n",
        "\n",
        "    # Iterate through each sequence of 1s and 0s, and if the sum of predicted anomalies\n",
        "    # in that sequence exceeds the PA%K ratio, set all elements in that sequence to 1.\n",
        "    for i in range(len(one_start_idx)):\n",
        "        if predicts[one_start_idx[i]:zero_start_idx[i]].sum() > k / 100 * (zero_start_idx[i] - one_start_idx[i]):\n",
        "            predicts[one_start_idx[i]:zero_start_idx[i]] = 1\n",
        "\n",
        "    return predicts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeF2lrzg3gap"
      },
      "source": [
        "##### PA@k plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-fP7G_13h7-"
      },
      "outputs": [],
      "source": [
        "scores = {\n",
        "    'ENC-DEC':{\n",
        "        'anomaly_scores_norm' : scores_encdec['anomaly_scores_norm'][-scores_hif_supervised['anomaly_scores_norm'].shape[0]:],\n",
        "        'threshold' : 0.32\n",
        "    },\n",
        "    'HIF_supervised':{\n",
        "        'anomaly_scores_norm' : scores_hif_supervised['anomaly_scores_norm'],\n",
        "        'threshold' : 0.56\n",
        "    },\n",
        "    'LSTM-AD':{\n",
        "        'anomaly_scores_norm' : scores_lstm['anomaly_scores_norm'][-scores_hif_supervised['anomaly_scores_norm'].shape[0]:],\n",
        "        'threshold' : 0.07\n",
        "    },\n",
        "    'HIF_unsupervised':{\n",
        "        'anomaly_scores_norm' : scores_hif_unsupervised['anomaly_scores_norm'][-scores_hif_supervised['anomaly_scores_norm'].shape[0]:],\n",
        "        'threshold' : 0.63\n",
        "    },\n",
        "    'K-Means':{\n",
        "        'anomaly_scores_norm' : scores_kmeans['anomaly_scores_norm'][-scores_hif_supervised['anomaly_scores_norm'].shape[0]:],\n",
        "        'threshold' : 0.31\n",
        "    }\n",
        "}\n",
        "\n",
        "for key in scores.keys():\n",
        "    threshold = scores[key]['threshold']\n",
        "    f1pa_k = [sklearn.metrics.f1_score(scores_hif_supervised['true_labels'], pak(scores[key]['anomaly_scores_norm'], scores_hif_supervised['true_labels'], threshold, k=k)) for k in range(0, 101)]\n",
        "    f1pa_k = np.array(f1pa_k)\n",
        "    area_trapz = np.trapz(f1pa_k, dx=0.01)\n",
        "    plt.plot(range(len(f1pa_k)), f1pa_k, label=f'{key} (AUC: {area_trapz:.2f})')\n",
        "    plt.fill_between(range(0, 101), f1pa_k, alpha=0.3)\n",
        "\n",
        "plt.grid(True, linestyle='-')\n",
        "plt.xlabel('K')\n",
        "plt.ylabel('F1$PA_{\\%K}$')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
